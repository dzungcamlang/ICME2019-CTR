{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "import scipy.special as special\n",
    "from pandas import DataFrame, Series\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from csv import DictReader\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "np.random.seed(2019)\n",
    "random.seed(2019)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_cont(train_df,test_df,f):    \n",
    "    \"\"\"\n",
    "    data=train_df.append(test_df)\n",
    "    group=data.groupby(f).size()\n",
    "    train_df[f+'_all_cont']=train_df[f].apply(lambda x:group[x])\n",
    "    test_df[f+'_all_cont']=test_df[f].apply(lambda x:group[x])\n",
    "    \"\"\"\n",
    "    print(\"all cont:\",f)\n",
    "    dic={}\n",
    "    for item in train_df[f].values:\n",
    "        try:\n",
    "            dic[item]+=1\n",
    "        except:\n",
    "            dic[item]=1    \n",
    "    for item in test_df[f].values:\n",
    "        try:\n",
    "            dic[item]+=1\n",
    "        except:\n",
    "            dic[item]=1     \n",
    "    cont=[]\n",
    "    for item in train_df[f].values:\n",
    "        cont.append(dic[item])        \n",
    "    train_df[f+'_all_cont']=cont\n",
    "    print('train','done')\n",
    "    \n",
    "    cont=[]\n",
    "    for item in test_df[f].values:\n",
    "        cont.append(dic[item])        \n",
    "    test_df[f+'_all_cont']=cont\n",
    "    print('test','done')            \n",
    "    print(f+'_all_cont')\n",
    "    print('avg of cont',np.mean(train_df[f+'_all_cont']),np.mean(test_df[f+'_all_cont']))   \n",
    "    \n",
    "def day_cont(train_df,test_df,f):\n",
    "    print(\"day cont:\",f)\n",
    "    dic={}\n",
    "    cont=[]\n",
    "    dics=[]\n",
    "    day=0\n",
    "    for item in train_df[['day',f]].values:\n",
    "        item[0]=int(item[0])\n",
    "        if day!=item[0]:\n",
    "            dics.append(dic)\n",
    "            dic={}\n",
    "            day+=1\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]]+=1\n",
    "        except:\n",
    "            dic[item[1]]=1\n",
    "    dics.append(dic)        \n",
    "    day=0\n",
    "    dic=dics[day]\n",
    "    for item in train_df[['day',f]].values:\n",
    "        item[0]=int(item[0])\n",
    "        if day!=item[0]:\n",
    "            day+=1 \n",
    "            dic=dics[day]\n",
    "            print(day) \n",
    "        cont.append(dic[item[1]])        \n",
    "    train_df[f+'_day_cont']=cont\n",
    "    print('train','done')\n",
    "    \n",
    "    dic={}\n",
    "    for item in test_df[f].values:\n",
    "        try:\n",
    "            dic[item]+=1\n",
    "        except:\n",
    "            dic[item]=1            \n",
    "    cont=[]\n",
    "    for item in test_df[f].values:\n",
    "        cont.append(dic[item])\n",
    "    test_df[f+'_day_cont']=cont    \n",
    "    print('test','done')\n",
    "    print(f+'_day_cont')\n",
    "    print('avg of cont',np.mean(train_df[f+'_day_cont']),np.mean(test_df[f+'_day_cont']))   \n",
    "    \n",
    "def combine(train_df,test_df,f1,f2):\n",
    "    train_df[f1+'_'+f2]=train_df[f1]*1e7+train_df[f2]\n",
    "    train_df[f1+'_'+f2]=train_df[f1+'_'+f2].astype(int)\n",
    "    test_df[f1+'_'+f2]=test_df[f1]*1e7 +test_df[f2]\n",
    "    test_df[f1+'_'+f2]=test_df[f1+'_'+f2].astype(int)\n",
    "    \n",
    "def kfold_static(train_df,test_df,f,label):\n",
    "    print(\"K-fold static:\",f+'_'+label)\n",
    "    #K-fold positive and negative num\n",
    "    avg_rate=train_df[label].mean()\n",
    "    num=len(train_df)//5\n",
    "    index=[0 for i in range(num)]+[1 for i in range(num)]+[2 for i in range(num)]+[3 for i in range(num)]+[4 for i in range(len(train_df)-4*num)]\n",
    "    random.shuffle(index)\n",
    "    train_df['index']=index\n",
    "\n",
    "        \n",
    "\n",
    "    dic=[{} for i in range(5)]\n",
    "    dic_all={}\n",
    "    for item in train_df[['index',f,label]].values:\n",
    "        try:\n",
    "            dic[item[0]][item[1]][item[2]]+=1\n",
    "        except:\n",
    "            dic[item[0]][item[1]]=[0,0]\n",
    "            dic[item[0]][item[1]][item[2]]+=1\n",
    "        try:\n",
    "            dic_all[item[1]][item[2]]+=1\n",
    "        except:\n",
    "            dic_all[item[1]]=[0,0]\n",
    "            dic_all[item[1]][item[2]]+=1\n",
    "    print(\"static done!\")\n",
    "                \n",
    "    positive=[]\n",
    "    negative=[]\n",
    "    rate=[]\n",
    "    for item in train_df[['index',f]].values:\n",
    "        n,p=dic_all[item[1]]\n",
    "        try:\n",
    "            p-=dic[item[0]][item[1]][1]\n",
    "            n-=dic[item[0]][item[1]][0] \n",
    "        except:\n",
    "            pass\n",
    "        if p==0 and n==0:\n",
    "            positive.append(-1)\n",
    "            negative.append(-1)\n",
    "            rate.append(avg_rate)\n",
    "        else:\n",
    "            positive.append(p)\n",
    "            negative.append(n)\n",
    "            rate.append(p/(p+n))  \n",
    "            \n",
    "    train_df[f+'_'+label+'_positive_num']=positive\n",
    "    train_df[f+'_'+label+'_negative_num']=negative\n",
    "    train_df[f+'_'+label+'_rate']=rate\n",
    "    print(\"train done!\")\n",
    "    #for test\n",
    "    positive=[]\n",
    "    negative=[]\n",
    "    rate=[]\n",
    "    for uid in test_df[f].values:\n",
    "        p=0\n",
    "        n=0\n",
    "        try:\n",
    "            p=dic_all[uid][1]\n",
    "            n=dic_all[uid][0]\n",
    "        except:\n",
    "            pass\n",
    "        if p==0 and n==0:\n",
    "            positive.append(-1)\n",
    "            negative.append(-1)\n",
    "            rate.append(avg_rate)\n",
    "        else:\n",
    "            positive.append(p)\n",
    "            negative.append(n)\n",
    "            rate.append(p/(p+n))            \n",
    "        \n",
    "    test_df[f+'_'+label+'_positive_num']=positive\n",
    "    test_df[f+'_'+label+'_negative_num']=negative  \n",
    "    test_df[f+'_'+label+'_rate']=rate\n",
    "    print(\"test done!\")\n",
    "    del train_df['index']\n",
    "    print(f+'_'+label+'_positive_num')\n",
    "    print(f+'_'+label+'_negative_num')\n",
    "    print(f+'_'+label+'_rate')\n",
    "    print('avg of positive num',np.mean(train_df[f+'_'+label+'_positive_num']),np.mean(test_df[f+'_'+label+'_positive_num']))\n",
    "    print('avg of negative num',np.mean(train_df[f+'_'+label+'_negative_num']),np.mean(test_df[f+'_'+label+'_negative_num']))\n",
    "    print('avg of rate',np.mean(train_df[f+'_'+label+'_rate']),np.mean(test_df[f+'_'+label+'_rate']))\n",
    "\n",
    "    \n",
    "    \n",
    "def w2v(train_df,test_df,f,flag,L):\n",
    "    print(\"w2v:\",f)\n",
    "    sentence=[]\n",
    "    dic={}\n",
    "    day=0\n",
    "    for item in train_df[['day','uid',f]].values:\n",
    "        if day!=item[0]:\n",
    "            for key in dic:\n",
    "                sentence.append(dic[key])\n",
    "            dic={}\n",
    "            day=item[0]\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]].append(str(item[2]))\n",
    "        except:\n",
    "            dic[item[1]]=[str(item[2])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    dic={}       \n",
    "    for item in test_df[['uid',f]].values:\n",
    "        try:\n",
    "            dic[item[0]].append(str(item[1]))\n",
    "        except:\n",
    "            dic[item[0]]=[str(item[1])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    print(len(sentence))\n",
    "    print('training...')\n",
    "    random.shuffle(sentence)\n",
    "    if f=='item_id':\n",
    "        model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=50)\n",
    "    else:\n",
    "        model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=10)\n",
    "    print('outputing...')\n",
    "\n",
    "\n",
    "    values=set(train_df[f].values)|set(test_df[f].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model[str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f]\n",
    "    for i in range(L):\n",
    "        names.append(names[0]+'_embedding_'+str(L)+'_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' + f +'_'+flag +'_w2v_'+str(L)+'.pkl') \n",
    "    \n",
    "def w2v_1(train_df,test_df,f,flag,L):\n",
    "    print(\"w2v:\",f)\n",
    "    sentence=[]\n",
    "    dic={}\n",
    "    day=0\n",
    "    for item in train_df[['day','author_id',f]].values:\n",
    "        if day!=item[0]:\n",
    "            for key in dic:\n",
    "                sentence.append(dic[key])\n",
    "            dic={}\n",
    "            day=item[0]\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]].append(str(item[2]))\n",
    "        except:\n",
    "            dic[item[1]]=[str(item[2])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    dic={}       \n",
    "    for item in test_df[['author_id',f]].values:\n",
    "        try:\n",
    "            dic[item[0]].append(str(item[1]))\n",
    "        except:\n",
    "            dic[item[0]]=[str(item[1])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    print(len(sentence))\n",
    "    print('training...')\n",
    "    random.shuffle(sentence)\n",
    "    model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=10)\n",
    "    print('outputing...')\n",
    "\n",
    "\n",
    "    values=set(train_df[f].values)|set(test_df[f].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model[str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f]\n",
    "    for i in range(L):\n",
    "        names.append(names[0]+'_embedding_'+str(L)+'_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' + f +'_'+flag +'_w2v_'+str(L)+'.pkl') \n",
    "    \n",
    "def w2v_2(train_df,test_df,f,flag):\n",
    "    print(\"w2v:\",f)\n",
    "    L=10\n",
    "    sentence=[]\n",
    "    dic={}\n",
    "    day=0\n",
    "    for item in train_df[['day','item_id',f]].values:\n",
    "        if day!=item[0]:\n",
    "            for key in dic:\n",
    "                sentence.append(dic[key])\n",
    "            dic={}\n",
    "            day=item[0]\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]].append(str(item[2]))\n",
    "        except:\n",
    "            dic[item[1]]=[str(item[2])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    dic={}       \n",
    "    for item in test_df[['item_id',f]].values:\n",
    "        try:\n",
    "            dic[item[0]].append(str(item[1]))\n",
    "        except:\n",
    "            dic[item[0]]=[str(item[1])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    print(len(sentence))\n",
    "    print('training...')\n",
    "    random.shuffle(sentence)\n",
    "    model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=10)\n",
    "    print('outputing...')\n",
    "\n",
    "\n",
    "    values=set(train_df[f].values)|set(test_df[f].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model[str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f]\n",
    "    for i in range(L):\n",
    "        names.append(names[0]+'_item_id_embedding_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/item_id_' + f +'_'+flag +'_w2v.pkl') \n",
    "    \n",
    "  \n",
    "    \n",
    "def title_w2v(train_df,test_df):\n",
    "    L=10\n",
    "    sentence=[]\n",
    "    for item in train_df['title_keys'].values:\n",
    "        sentence.append(item.split())\n",
    "    for item in test_df['title_keys'].values:\n",
    "        sentence.append(item.split())    \n",
    "    print(len(sentence))\n",
    "    print('training...')\n",
    "    random.shuffle(sentence)\n",
    "    model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=10)\n",
    "    print('outputing...')\n",
    "    \n",
    "    w2v=[]\n",
    "    for item in train_df['title_keys'].values:\n",
    "        array=np.zeros(L)\n",
    "        words=item.split()\n",
    "        size=len(words)\n",
    "        for word in words:\n",
    "            array+=model[word]/size\n",
    "        w2v.append(array)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=['word_embedding_'+str(i) for i in range(L)]\n",
    "    out_df.columns = names\n",
    "\n",
    "    for n in names:\n",
    "        train_df[n]=list(out_df[n].values)\n",
    "    \n",
    "    \n",
    "    w2v=[]\n",
    "    for item in test_df['title_keys'].values:\n",
    "        array=np.zeros(L)\n",
    "        words=item.split()\n",
    "        size=len(words)\n",
    "        for word in words:\n",
    "            array+=model[word]/size\n",
    "        w2v.append(array)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=['word_embedding_'+str(i) for i in range(L)]\n",
    "    out_df.columns = names\n",
    "\n",
    "    for n in names:\n",
    "        test_df[n]=list(out_df[n].values)\n",
    "    print(train_df)\n",
    "    print(test_df)\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "def var_mean(train_df,test_df):\n",
    "    data=train_df[['uid','author_id']].append(test_df[['uid','author_id']])\n",
    "    group=data[['uid','author_id']].groupby('uid')\n",
    "    group=group.apply(lambda x:np.var(np.fft.fft(list(Counter(list(x['author_id'])).values()))))\n",
    "    train_df['uid_num_of_author_fft_var']=train_df['uid'].apply(lambda x:group[x])\n",
    "    test_df['uid_num_of_author_fft_var']=test_df['uid'].apply(lambda x:group[x])\n",
    "    print(train_df[train_df['finish']==1]['uid_num_of_author_fft_var'].mean())\n",
    "    print(train_df[train_df['finish']==0]['uid_num_of_author_fft_var'].mean())\n",
    "\n",
    "    group=data[['uid','author_id']].groupby('uid')\n",
    "    group=group.apply(lambda x:np.var(list(Counter(list(x['author_id'])).values())))\n",
    "    train_df['uid_num_of_author_var']=train_df['uid'].apply(lambda x:group[x])\n",
    "    test_df['uid_num_of_author_var']=test_df['uid'].apply(lambda x:group[x])\n",
    "    print(train_df[train_df['finish']==1]['uid_num_of_author_var'].mean())\n",
    "    print(train_df[train_df['finish']==0]['uid_num_of_author_var'].mean())\n",
    "\n",
    "    group=data[['uid','author_id']].groupby('uid')\n",
    "    group=group.apply(lambda x:np.mean(list(Counter(list(x['author_id'])).values())))\n",
    "    train_df['uid_num_of_author_mean']=train_df['uid'].apply(lambda x:group[x])\n",
    "    test_df['uid_num_of_author_mean']=test_df['uid'].apply(lambda x:group[x])\n",
    "    print(train_df[train_df['finish']==1]['uid_num_of_author_mean'].mean())\n",
    "    print(train_df[train_df['finish']==0]['uid_num_of_author_mean'].mean())\n",
    "    \n",
    "    \n",
    "def did_features(train_df,test_df):\n",
    "    data=train_df[['uid','did','author_id']].append(test_df[['uid','did','author_id']])\n",
    "\n",
    "    group=data[['uid','did']].groupby('uid')\n",
    "    group=group.apply(lambda x: len(set(x['did'])))\n",
    "    train_df['uid_has_num_of_did']=train_df['uid'].apply(lambda x: group[x])\n",
    "    test_df['uid_has_num_of_did']=test_df['uid'].apply(lambda x: group[x])\n",
    "\n",
    "    group=data[['did','author_id']].groupby('did')\n",
    "    group=group.apply(lambda x: len(set(x['author_id'])))\n",
    "    train_df['did_has_num_of_author']=train_df['did'].apply(lambda x: group[x])\n",
    "    test_df['did_has_num_of_author']=test_df['did'].apply(lambda x: group[x])\n",
    "\n",
    "def author_features(train_df,test_df):\n",
    "    data=train_df.append(test_df)\n",
    "    groupby=data[['author_id','item_id']].drop_duplicates().groupby('author_id')\n",
    "    groupby=groupby.apply(lambda x: len(set(x['item_id'])))\n",
    "    print('author_include_num_of_item')\n",
    "    train_df['author_include_num_of_item']=train_df['author_id'].apply(lambda x:groupby[x])\n",
    "    test_df['author_include_num_of_item']=test_df['author_id'].apply(lambda x:groupby[x])\n",
    "    print('avg of author_include_num_of_item',np.mean(train_df['author_include_num_of_item']),\\\n",
    "          np.mean(test_df['author_include_num_of_item']))  \n",
    "    \n",
    "    groupby=data[['author_id','uid']].drop_duplicates().groupby('author_id')\n",
    "    groupby=groupby.apply(lambda x: len(set(x['uid'])))\n",
    "    print('author_include_num_of_uid')\n",
    "    train_df['author_include_num_of_uid']=train_df['author_id'].apply(lambda x:groupby[x])\n",
    "    test_df['author_include_num_of_uid']=test_df['author_id'].apply(lambda x:groupby[x])\n",
    "    print('avg of author_include_num_of_uid',np.mean(train_df['author_include_num_of_uid']),\\\n",
    "          np.mean(test_df['author_include_num_of_uid']))        \n",
    "def uid_author_01(train_df,test_df):\n",
    "    dic_1={}\n",
    "    dic_2={}\n",
    "    dic={}\n",
    "    temp=[]\n",
    "    day=0\n",
    "    for item in train_df[['uid','author_id','finish','day']].values:\n",
    "        if day!=item[3]:\n",
    "            dic_1=dic_2.copy()\n",
    "            dic_2=dic.copy()\n",
    "            day=item[3]\n",
    "            print(day)\n",
    "        t=tuple(item[:2])\n",
    "        try:\n",
    "            if item[2]==0:\n",
    "                dic[t]+='0'\n",
    "            else:\n",
    "                dic[t]+='1'\n",
    "        except:\n",
    "            dic[t]=''\n",
    "            if item[2]==0:\n",
    "                dic[t]+='0'\n",
    "            else:\n",
    "                dic[t]+='1'        \n",
    "        try:\n",
    "            temp.append(dic_1[t][-4:])\n",
    "        except:\n",
    "            temp.append('-1')\n",
    "\n",
    "    train_df['01']=temp\n",
    "    total=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','author_id']].values:\n",
    "        item=tuple(item)\n",
    "        try:\n",
    "            temp.append(dic[item][-4:])\n",
    "        except:\n",
    "            temp.append('-1')\n",
    "    test_df['01']=temp\n",
    "    total+=temp\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(total)\n",
    "    train_df['01'] = lbl.transform(train_df['01'])\n",
    "    test_df['01'] = lbl.transform(test_df['01'])\n",
    "    \n",
    "def last_author_id(train_df,test_df):\n",
    "    dic={}\n",
    "    temp=[]\n",
    "    for item in train_df[['uid','author_id']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    train_df['last_author']=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','author_id']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    test_df['last_author']=temp    \n",
    "    \n",
    "def last_item_id(train_df,test_df):\n",
    "    dic={}\n",
    "    temp=[]\n",
    "    for item in train_df[['uid','item_id']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    train_df['last_item_id']=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','item_id']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    test_df['last_item_id']=temp     \n",
    "    \n",
    "def last_did(train_df,test_df):\n",
    "    dic={}\n",
    "    temp=[]\n",
    "    for item in train_df[['uid','did']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    train_df['last_did']=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','did']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    test_df['last_did']=temp  \n",
    "    \n",
    "class BayesianSmoothing(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def update(self, imps, clks, iter_num, epsilon):\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "            if abs(new_alpha - self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n",
    "                break\n",
    "            print(new_alpha, new_beta, i)\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "        numerator_alpha = 0.0\n",
    "        numerator_beta = 0.0\n",
    "        denominator = 0.0\n",
    "        for i in range(len(imps)):\n",
    "            numerator_alpha += (special.digamma(clks[i] + alpha) - special.digamma(alpha))\n",
    "            numerator_beta += (special.digamma(imps[i] - clks[i] + beta) - special.digamma(beta))\n",
    "            denominator += (special.digamma(imps[i] + alpha + beta) - special.digamma(alpha + beta))\n",
    "\n",
    "        return alpha * (numerator_alpha / denominator), beta * (numerator_beta / denominator)\n",
    "    \n",
    "def rate_statistic(train_df,test_df,label,f,num):\n",
    "    bs = BayesianSmoothing(1, 1)\n",
    "    l = list(set(train_df[f].values))\n",
    "    dic = dict(Counter(train_df[f].values))\n",
    "    I = []\n",
    "    C = []\n",
    "    for idx in l:\n",
    "        I.append(dic[idx])\n",
    "    positive = train_df[train_df[label] == 1]\n",
    "    dic2 = dict(Counter(positive[f].values))\n",
    "    for idx in l:\n",
    "        if idx not in dic2:\n",
    "            C.append(0)\n",
    "        else:\n",
    "            C.append(dic2[idx])\n",
    "    bs.update(I, C, num, 0.0000000001)\n",
    "    alpha, beta=bs.alpha, bs.beta\n",
    "    \n",
    "    groupby={}\n",
    "    for item in train_df[[f,label]].values:\n",
    "        try:\n",
    "            groupby[item[0]][item[1]]+=1\n",
    "        except:\n",
    "            groupby[item[0]]=[0,0]\n",
    "            groupby[item[0]][item[1]]+=1\n",
    "    a = []\n",
    "    b = []\n",
    "    for i in train_df[f].values:\n",
    "        try:\n",
    "            label1= groupby[i][1]\n",
    "            label0= groupby[i][0]\n",
    "        except:\n",
    "            label1 = 0\n",
    "            label0 = 0\n",
    "        a.append((label1 + alpha) / (label0 + label1 + alpha + beta))\n",
    "    for i in test_df[f].values:\n",
    "        try:\n",
    "            label1= groupby[i][1]\n",
    "            label0= groupby[i][0]\n",
    "        except:\n",
    "            label1 = 0\n",
    "            label0 = 0\n",
    "        b.append((label1 + alpha) / (label0 + label1 + alpha + beta))\n",
    "    train_df[f +'_'+label+ '_smooth_rate'] = a\n",
    "    test_df[f + '_'+label+'_smooth_rate'] = b\n",
    "    print(f +'_'+label+ '_smooth_rate')\n",
    "    print(train_df[f +'_'+label+ '_smooth_rate'].head())\n",
    "    print(train_df[f +'_'+label+ '_smooth_rate'].mean(),test_df[f +'_'+label+ '_smooth_rate'].mean())  \n",
    " \n",
    "\n",
    "        \n",
    "\n",
    "def uid_item_time_stamp(train_df,test_df):\n",
    "    data=train_df.append(test_df)\n",
    "    groupby=data.groupby('uid')['item_id'].mean()\n",
    "    train_df['mean_id']=train_df['uid'].apply(lambda x: groupby[x])\n",
    "    test_df['mean_id']=test_df['uid'].apply(lambda x: groupby[x])\n",
    "    train_df['id_mean_differ']=train_df['item_id']-train_df['mean_id']\n",
    "    test_df['id_mean_differ']=test_df['item_id']-test_df['mean_id']\n",
    "    print(\"done 1!\")\n",
    "    groupby=data.groupby('uid')['item_id'].min()\n",
    "    train_df['min_id']=train_df['uid'].apply(lambda x: groupby[x])\n",
    "    test_df['min_id']=test_df['uid'].apply(lambda x: groupby[x])\n",
    "    train_df['id_min_differ']=train_df['item_id']-train_df['min_id']\n",
    "    test_df['id_min_differ']=test_df['item_id']-test_df['min_id']\n",
    "    print(\"done 2!\")\n",
    "\n",
    "    groupby=data.groupby('uid')['item_id'].max()\n",
    "    train_df['max_id']=train_df['uid'].apply(lambda x: groupby[x])\n",
    "    test_df['max_id']=test_df['uid'].apply(lambda x: groupby[x])\n",
    "    train_df['id_max_differ']=train_df['item_id']-train_df['max_id']\n",
    "    test_df['id_max_differ']=test_df['item_id']-test_df['max_id']\n",
    "    print(\"done 3!\")\n",
    "    \n",
    "def deepwalk(train_df,test_df,f1,f2,flag,L):\n",
    "    print(\"deepwalk:\",f1,f2)\n",
    "    dic={}\n",
    "\n",
    "    for item in train_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic['item_'+str(item[1])].add('user_'+str(item[0]))\n",
    "        except:\n",
    "            dic['item_'+str(item[1])]=set(['user_'+str(item[0])])\n",
    "        try:\n",
    "            dic['user_'+str(item[0])].add('item_'+str(item[1]))\n",
    "        except:\n",
    "            dic['user_'+str(item[0])]=set(['item_'+str(item[1])])\n",
    "\n",
    "    for item in test_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic['item_'+str(item[1])].add('user_'+str(item[0]))\n",
    "        except:\n",
    "            dic['item_'+str(item[1])]=set(['user_'+str(item[0])])\n",
    "        try:\n",
    "            dic['user_'+str(item[0])].add('item_'+str(item[1]))\n",
    "        except:\n",
    "            dic['user_'+str(item[0])]=set(['item_'+str(item[1])])\n",
    "    print(\"creating\")        \n",
    "    path_length=10        \n",
    "    sentences=[]\n",
    "    length=[]\n",
    "    for key in dic:\n",
    "        sentence=[key]\n",
    "        while len(sentence)!=path_length:\n",
    "            key=random.sample(dic[sentence[-1]],1)[0]\n",
    "            if len(sentence)>=2 and key == sentence[-2]:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(key)\n",
    "\n",
    "        sentences.append(sentence)\n",
    "        length.append(len(sentence))\n",
    "        if len(sentences)%100000==0:\n",
    "            print(len(sentences))\n",
    "    print(np.mean(length))\n",
    "    print(len(sentences))\n",
    "    print('training...')\n",
    "    random.shuffle(sentences)\n",
    "    model = Word2Vec(sentences, size=L, window=4,min_count=1,sg=1, workers=10,iter=20)\n",
    "    print('outputing...')\n",
    "    \n",
    "    values=set(train_df[f1].values)|set(test_df[f1].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model['user_'+str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f1]\n",
    "    for i in range(L):\n",
    "        names.append(f1+'_'+ f2+'_'+names[0]+'_deepwalk_embedding_'+str(L)+'_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' +f1+'_'+ f2+'_'+f1 +'_'+flag +'_deepwalk_'+str(L)+'.pkl') \n",
    "    \n",
    "    values=set(train_df[f2].values)|set(test_df[f2].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model['item_'+str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f2]\n",
    "    for i in range(L):\n",
    "        names.append(f1+'_'+ f2+'_'+names[0]+'_deepwalk_embedding_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' +f1+'_'+ f2+'_'+f2 +'_'+flag +'_deepwalk_'+str(L)+'.pkl') \n",
    "    \n",
    "def get_agg_features(train_df,test_df,f1,f2,agg):\n",
    "    if type(f1)==str:\n",
    "        f1=[f1]\n",
    "    data=train_df[f1+[f2]].append(test_df[f1+[f2]])\n",
    "    if agg == \"count\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].count()).reset_index()\n",
    "    elif agg==\"mean\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].mean()).reset_index()\n",
    "    elif agg==\"nunique\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].nunique()).reset_index()\n",
    "    elif agg==\"max\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].max()).reset_index()\n",
    "    elif agg==\"min\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].min()).reset_index()\n",
    "    elif agg==\"sum\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].sum()).reset_index()\n",
    "    elif agg==\"std\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].std()).reset_index()\n",
    "    elif agg==\"median\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].median()).reset_index()\n",
    "    elif agg==\"skew\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].skew()).reset_index()\n",
    "    elif agg==\"unique_mean\":\n",
    "        group=data.groupby(f1)\n",
    "        group=group.apply(lambda x:np.mean(list(Counter(list(x[f2])).values())))\n",
    "        tmp = pd.DataFrame(group.reset_index())\n",
    "    elif agg==\"unique_var\":\n",
    "        group=data.groupby(f1)\n",
    "        group=group.apply(lambda x:np.var(list(Counter(list(x[f2])).values())))\n",
    "        tmp = pd.DataFrame(group.reset_index())\n",
    "    else:\n",
    "        raise \"agg error\"\n",
    "    tmp.columns = f1+['_'.join(f1)+\"_\"+f2+\"_\"+agg]\n",
    "    print('_'.join(f1)+\"_\"+f2+\"_\"+agg)\n",
    "    train_df=train_df.merge(tmp, on=f1, how='left')\n",
    "    test_df=test_df.merge(tmp, on=f1, how='left')\n",
    "    del tmp\n",
    "    del data\n",
    "    gc.collect()\n",
    "    print(train_df.shape,test_df.shape)\n",
    "    return train_df,test_df  \n",
    "    \n",
    "    \n",
    "def pagerank(train_df,test_df,f1,f2,alpha,iter):\n",
    "    print(\"Create Graph\",f1,f2)\n",
    "    M={}\n",
    "    dic={}\n",
    "    for item in train_df[[f1,f2]].append(test_df[[f1,f2]]).values:\n",
    "        if item[0] in dic:\n",
    "            last_item=dic[item[0]]\n",
    "            try:\n",
    "                M[last_item][item[1]]+=1\n",
    "            except:\n",
    "                M[last_item]=Counter()\n",
    "                M[last_item][item[1]]+=1\n",
    "        else:\n",
    "            last_item=\"begin\"\n",
    "            try:\n",
    "                M[last_item][item[1]]+=1\n",
    "            except:\n",
    "                M[last_item]=Counter()\n",
    "                M[last_item][item[1]]+=1            \n",
    "        dic[item[0]]=item[1]\n",
    "    for key in dic:\n",
    "        last_item=dic[key]\n",
    "        try:\n",
    "            M[last_item][\"end\"]+=1\n",
    "        except:\n",
    "            M[last_item]=Counter()\n",
    "            M[last_item][\"end\"]+=1            \n",
    "            \n",
    "    for key in M:\n",
    "        temp=M[key]\n",
    "        dic={}\n",
    "        cont=sum(temp.values())\n",
    "        for item in zip(temp.keys(),temp.values()):\n",
    "            dic[item[0]]=item[1]/cont\n",
    "        M[key]=dic\n",
    "    print(\"Run pageRank\")\n",
    "    keys=set(train_df[f2].values)|set(test_df[f2].values)\n",
    "    keys.add(\"begin\")\n",
    "    keys.add(\"end\")\n",
    "    U={}\n",
    "    for key in keys:\n",
    "        U[key]=1\n",
    "    U0=U.copy()\n",
    "    for _ in range(iter):\n",
    "        loss=0\n",
    "        new_U={}\n",
    "        for key in M:\n",
    "            for k in M[key]:\n",
    "                try:\n",
    "                    new_U[k]+=U[key]*M[key][k]\n",
    "                except:\n",
    "                    new_U[k]=U[key]*M[key][k]\n",
    "        for key in U0:\n",
    "            try:\n",
    "                new_U[key]\n",
    "            except:\n",
    "                new_U[key]=0\n",
    "            tmp=alpha*new_U[key]+(1-alpha)*U0[key]\n",
    "            loss+=abs(tmp-U[key])\n",
    "            U[key]=tmp\n",
    "        print(_+1,loss)\n",
    "        if loss<1e-1:\n",
    "            break\n",
    "    print(f1+'_'+f2+'_pagerank')\n",
    "    train_df[f1+'_'+f2+'_pagerank']=train_df[f2].apply(lambda x: U[x])\n",
    "    test_df[f1+'_'+f2+'_pagerank']=test_df[f2].apply(lambda x: U[x])\n",
    "    print(train_df[train_df['finish']==0][f1+'_'+f2+'_pagerank'].mean(),train_df[train_df['finish']==1][f1+'_'+f2+'_pagerank'].mean())        \n",
    "    \n",
    "    \n",
    "def title_mean(train_df,test_df):\n",
    "    temp=[]\n",
    "    for item in train_df[['title_keys','title_values']].values:\n",
    "        mean = 0\n",
    "        if item[0]==\"empty\":\n",
    "            temp.append(-1)\n",
    "        else:\n",
    "            for t1,t2 in zip(item[0].split(),item[1]):\n",
    "                mean+=int(t1)*t2\n",
    "            mean/=sum(item[1])\n",
    "            temp.append(mean)\n",
    "    train_df['title_mean']=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['title_keys','title_values']].values:\n",
    "        mean = 0\n",
    "        if item[0]==\"empty\":\n",
    "            temp.append(-1)\n",
    "        else:\n",
    "            for t1,t2 in zip(item[0].split(),item[1]):\n",
    "                mean+=int(t1)*t2\n",
    "            mean/=sum(item[1])\n",
    "            temp.append(mean)\n",
    "    test_df['title_mean']=temp    \n",
    "    print('title_mean')\n",
    "    print(train_df['title_mean'].mean(),test_df['title_mean'].mean()) \n",
    "    \n",
    "def titile_kfold_static(train_df,test_df,label):\n",
    "    print(\"K-fold static:\",'title_'+label)\n",
    "    #K-fold positive and negative num\n",
    "    avg_rate=train_df[label].mean()\n",
    "    num=len(train_df)//5\n",
    "    index=[0 for i in range(num)]+[1 for i in range(num)]+[2 for i in range(num)]+[3 for i in range(num)]+[4 for i in range(len(train_df)-4*num)]\n",
    "    random.shuffle(index)\n",
    "    train_df['index']=index\n",
    "\n",
    "        \n",
    "\n",
    "    dic=[{} for i in range(5)]\n",
    "    dic_all={}\n",
    "    for item in train_df[['index','title_keys',label]].values:\n",
    "        for t in item[1].split():\n",
    "            try:\n",
    "                dic[item[0]][t][item[2]]+=1\n",
    "            except:\n",
    "                dic[item[0]][t]=[0,0]\n",
    "                dic[item[0]][t][item[2]]+=1\n",
    "            try:\n",
    "                dic_all[t][item[2]]+=1\n",
    "            except:\n",
    "                dic_all[t]=[0,0]\n",
    "                dic_all[t][item[2]]+=1\n",
    "    print(\"static done!\")\n",
    "                \n",
    "\n",
    "\n",
    "    min_rate=[]\n",
    "    max_rate=[]\n",
    "    mean_rate=[]\n",
    "    for item in train_df[['index','title_keys']].values:\n",
    "        temp=[]\n",
    "        for t in item[1].split():\n",
    "            n=0\n",
    "            p=0\n",
    "            for j in range(5):\n",
    "                if j!=item[0]:\n",
    "                    try:\n",
    "                        p+=dic[j][t][1]\n",
    "                        n+=dic[j][t][0] \n",
    "                    except:\n",
    "                        pass\n",
    "            if p==0 and n==0:\n",
    "                temp.append(avg_rate)\n",
    "            else:\n",
    "                temp.append(p/(p+n))\n",
    "        min_rate.append(min(temp))\n",
    "        max_rate.append(max(temp))\n",
    "        mean_rate.append(np.mean(temp))\n",
    "    \n",
    "            \n",
    "    train_df['title_max_rate_'+label]=max_rate\n",
    "    train_df['title_min_rate_'+label]=min_rate\n",
    "    train_df['title_mean_rate_'+label]=mean_rate\n",
    "    \n",
    "    print(\"train done!\")\n",
    "    #for test\n",
    "    min_rate=[]\n",
    "    max_rate=[]\n",
    "    mean_rate=[]\n",
    "    for item in test_df['title_keys'].values:\n",
    "        temp=[]\n",
    "        for t in item.split():\n",
    "            n=0\n",
    "            p=0\n",
    "            for j in range(5):\n",
    "                try:\n",
    "                    p+=dic[j][t][1]\n",
    "                    n+=dic[j][t][0] \n",
    "                except:\n",
    "                    pass\n",
    "            if p==0 and n==0:\n",
    "                temp.append(avg_rate)\n",
    "            else:\n",
    "                temp.append(p/(p+n))\n",
    "        min_rate.append(min(temp))\n",
    "        max_rate.append(max(temp))\n",
    "        mean_rate.append(np.mean(temp)) \n",
    "\n",
    "        \n",
    "    test_df['title_max_rate_'+label]=max_rate\n",
    "    test_df['title_min_rate_'+label]=min_rate\n",
    "    test_df['title_mean_rate_'+label]=mean_rate\n",
    "    print(\"test done!\")\n",
    "    \n",
    "def trajectary(train_df,test_df,f1,f2):\n",
    "    print(\"trajectary:\",f1,f2)\n",
    "    temp=[]\n",
    "    dic={}\n",
    "    day=0\n",
    "    for item in train_df[['day',f1,f2]].values:\n",
    "        if day!=item[0]:\n",
    "            dic={}\n",
    "            day=item[0]\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]].append(str(item[2]))\n",
    "        except:\n",
    "            dic[item[1]]=[str(item[2])]\n",
    "        temp.append(' '.join(dic[item[1]][-20:]))\n",
    "        \n",
    "        \n",
    "    train_df[\"trajectary_\"+f1+\"_\"+f2]=temp\n",
    "    \n",
    "    temp=[]\n",
    "    dic={}       \n",
    "    for item in test_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic[item[0]].append(str(item[1]))\n",
    "        except:\n",
    "            dic[item[0]]=[str(item[1])]\n",
    "        temp.append(' '.join(dic[item[0]][-20:]))\n",
    "    \n",
    "    test_df[\"trajectary_\"+f1+\"_\"+f2]=temp\n",
    "    print(\"trajectary_\"+f1+\"_\"+f2)\n",
    "    print(train_df[\"trajectary_\"+f1+\"_\"+f2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition as sk_decomposition\n",
    "import pandas as pd\n",
    "def gbdt(train_df,test_df,flag,label):\n",
    "    print(\"GBDT\",label)\n",
    "    if label=='finish':\n",
    "        features=['uid','user_city','item_id','author_id','item_city','channel','music_id','did','video_duration',\n",
    "              'did_finish_positive_num', 'did_finish_negative_num', 'did_finish_rate', 'did_all_cont','did_day_cont',\n",
    "              'author_id_finish_positive_num', 'author_id_finish_negative_num', 'author_id_finish_rate','author_id_all_cont','author_id_day_cont',\n",
    "              'item_id_finish_positive_num', 'item_id_finish_negative_num', 'item_id_finish_rate','item_id_all_cont','item_id_day_cont',\n",
    "              'author_id_did_all_cont','author_include_num_of_item','author_include_num_of_uid','uid_num_of_author_mean','uid_num_of_author_var','uid_num_of_author_fft_var',\n",
    "              'title_cont','beauty_min','uid_has_num_of_did','mean_id','id_mean_differ','max_id','id_max_differ','min_id','id_min_differ',\n",
    "              'title_mean_rate_finish','title_max_rate_finish','title_min_rate_finish','title_mean',  \n",
    "              'uid_channel_title_mean_mean', 'uid_channel_title_mean_std', 'uid_channel_title_mean_skew',\n",
    "               'uid_item_id_pagerank', 'uid_author_id_pagerank', 'did_item_id_pagerank', 'did_author_id_pagerank', 'item_id_uid_pagerank', 'item_id_did_pagerank', 'author_id_uid_pagerank', 'author_id_did_pagerank']\n",
    "    else:\n",
    "        features=['uid','user_city','item_id','author_id','item_city','channel','music_id','did','video_duration','did_like_positive_num', 'did_like_negative_num', 'did_like_rate', 'did_all_cont','did_day_cont',\n",
    "                  'author_id_like_positive_num', 'author_id_like_negative_num','author_id_like_rate','author_id_all_cont','author_id_day_cont','item_id_like_positive_num', 'item_id_like_negative_num','item_id_like_rate','item_id_all_cont','item_id_day_cont',\n",
    "                  'uid_like_positive_num', 'uid_like_negative_num', 'uid_like_rate','user_city_like_positive_num', 'user_city_like_negative_num', 'user_city_like_rate','item_city_like_positive_num', 'item_city_like_negative_num', 'item_city_like_rate',\n",
    "                  'music_id_like_positive_num', 'music_id_like_negative_num', 'music_id_like_rate','author_include_num_of_item','author_include_num_of_uid',\n",
    "                  'uid_num_of_author_mean','uid_num_of_author_var','uid_num_of_author_fft_var','author_id_did_all_cont','title_cont','beauty_min','uid_has_num_of_did',\n",
    "                  'mean_id','id_mean_differ','max_id','id_max_differ','min_id','id_min_differ','title_mean_rate_like','uid_item_id_unique_mean', 'uid_author_id_unique_mean', \n",
    "                  'uid_channel_unique_mean', 'did_item_id_unique_mean', 'did_author_id_unique_mean', 'did_channel_unique_mean', 'uid_item_id_unique_var', 'uid_author_id_unique_var', 'uid_channel_unique_var', 'did_item_id_unique_var', \n",
    "                 'did_author_id_unique_var', 'did_channel_unique_var', 'title_mean', 'uid_title_mean_mean', 'uid_title_mean_std', 'uid_title_mean_skew', 'author_id_title_mean_mean', \n",
    "                 'author_id_title_mean_std', 'author_id_title_mean_skew', 'did_title_mean_mean', 'did_title_mean_std', 'did_title_mean_skew', 'uid_channel_title_mean_mean', 'uid_channel_title_mean_std', 'uid_channel_title_mean_skew',\n",
    "                 'uid_item_id_pagerank', 'uid_author_id_pagerank', 'did_item_id_pagerank', 'did_author_id_pagerank', 'item_id_uid_pagerank', 'item_id_did_pagerank', 'author_id_uid_pagerank', 'author_id_did_pagerank']\n",
    "    for col in ['author_id','did','music_id','item_id']:\n",
    "        df = pd.read_csv('deepwalk_data_128/user_'+col+'_uid_deepwalk_128.csv')\n",
    "        df = df.drop_duplicates(['uid'])\n",
    "        dim=32\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove('uid')\n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on='uid', how='left')\n",
    "        test_df = pd.merge(test_df, df1, on='uid', how='left')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "    print(\"done 1!\")\n",
    "    \n",
    "    for col in ['author_id','did','music_id','uid']:\n",
    "        df = pd.read_pickle('deepwalk_data_64/item_'+col+'_uid_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates(['item_id'])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove('item_id') \n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on='item_id', how='left')\n",
    "        test_df = pd.merge(test_df, df1, on='item_id', how='left')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "    print(\"done 2!\")\n",
    "    \n",
    "    for col in ['author_id','music_id','uid','item_id']:\n",
    "        df = pd.read_pickle('deepwalk_data_64/did_'+col+'_did_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates(['did'])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove('did') \n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on='did', how='left')\n",
    "        test_df = pd.merge(test_df, df1, on='did', how='left')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "    print(\"done 3!\")    \n",
    "    \n",
    "    for col in ['did','music_id','uid','item_id']:\n",
    "        df = pd.read_pickle('deepwalk_data_64/author_'+col+'_author_id_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates(['author_id'])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove('author_id')  \n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on='author_id', how='left')\n",
    "        test_df = pd.merge(test_df, df1, on='author_id', how='left')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "    print(\"done 4!\")    \n",
    "\n",
    "    for f1,f2 in [('uid','item_id'),('uid','author_id'),('did','item_id'),('did','author_id')]:\n",
    "        col=f1\n",
    "        df = pd.read_pickle( 'data/' +f1+'_'+ f2+'_'+col +'_'+flag +'_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates([col])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove(col)\n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on=col, how='left')\n",
    "        test_df = pd.merge(test_df, df1, on=col, how='left')\n",
    "        print(train_df.shape,test_df.shape) \n",
    "        \n",
    "        col=f2\n",
    "        df = pd.read_pickle( 'data/' +f1+'_'+ f2+'_'+col +'_'+flag +'_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates([col])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove(col)\n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on=col, how='left')\n",
    "        test_df = pd.merge(test_df, df1, on=col, how='left')\n",
    "        print(train_df.shape,test_df.shape)                 \n",
    "    print(\"done 5!\")\n",
    "\n",
    "    for col in ['video','audio','author_id','uid','did','item_id']:\n",
    "        if col in ['audio']:\n",
    "            df = pd.read_pickle( 'data/' + col + '_w2v_svd_64.pkl')\n",
    "            col='item_id'\n",
    "        elif col in ['video']:\n",
    "            df = pd.read_pickle( 'data/' + col + '_w2v_svd_32.pkl')\n",
    "            col='item_id'    \n",
    "        else:\n",
    "            df = pd.read_pickle( 'data/' + col + '_'+flag+'_w2v_128.pkl')\n",
    "            df = df.drop_duplicates([col])\n",
    "            dim=32\n",
    "            pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "            pca.fit(df[df.columns[1:]])\n",
    "            df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "            df1.columns=df.columns[1:dim+1]\n",
    "            df1[df.columns[0]]=df[df.columns[0]]\n",
    "            df=df1\n",
    "        df = df.drop_duplicates([col])\n",
    "        fs = list(df)\n",
    "        fs.remove(col)\n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df, on=col, how='left')\n",
    "        test_df = pd.merge(test_df, df, on=col, how='left')\n",
    "        print(train_df.shape,test_df.shape)  \n",
    "    print(\"done 6!\")\n",
    "    print(len(features),len(set(features)))\n",
    "    n_trees=30\n",
    "    print(\"training\")\n",
    "    clf = xgb.XGBClassifier(learning_rate=1, n_estimators=n_trees, max_depth=8,objective='binary:logistic',seed=0,nthread=-1)\n",
    "    \n",
    "    clf.fit(train_df[features], train_df[label])\n",
    "    print('train done!')   \n",
    "    out_df = pd.DataFrame(clf.apply(train_df[features]))\n",
    "    out_df.columns = [label+'_G' + str(i) for i in range(n_trees )]\n",
    "    out_df['ids']=train_df['ids']\n",
    "    out_df.to_pickle(\"data/gbdt_train_\"+label+'_'+flag+'.pkl')\n",
    "    out_df = pd.DataFrame(clf.apply(test_df[features]))\n",
    "    out_df.columns = [label+'_G' + str(i) for i in range(n_trees )]\n",
    "    out_df['ids']=test_df['ids']\n",
    "    out_df.to_pickle(\"data/gbdt_test_\"+label+'_'+flag+'.pkl')\n",
    "    print(\"convert done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_dev.pkl data/dev.pkl\n",
      "(14015958, 171) (2803191, 171)\n",
      "GBDT finish\n",
      "(14015958, 203) (2803191, 203)\n",
      "(14015958, 235) (2803191, 235)\n",
      "(14015958, 267) (2803191, 267)\n",
      "(14015958, 299) (2803191, 299)\n",
      "done 1!\n",
      "(14015958, 315) (2803191, 315)\n",
      "(14015958, 331) (2803191, 331)\n",
      "(14015958, 347) (2803191, 347)\n",
      "(14015958, 363) (2803191, 363)\n",
      "done 2!\n",
      "(14015958, 379) (2803191, 379)\n",
      "(14015958, 395) (2803191, 395)\n",
      "(14015958, 411) (2803191, 411)\n",
      "(14015958, 427) (2803191, 427)\n",
      "done 3!\n",
      "(14015958, 443) (2803191, 443)\n",
      "(14015958, 459) (2803191, 459)\n",
      "(14015958, 475) (2803191, 475)\n",
      "(14015958, 491) (2803191, 491)\n",
      "done 4!\n",
      "(14015958, 507) (2803191, 507)\n",
      "(14015958, 523) (2803191, 523)\n",
      "(14015958, 539) (2803191, 539)\n",
      "(14015958, 555) (2803191, 555)\n",
      "(14015958, 571) (2803191, 571)\n",
      "(14015958, 587) (2803191, 587)\n",
      "(14015958, 603) (2803191, 603)\n",
      "(14015958, 619) (2803191, 619)\n",
      "done 5!\n",
      "(14015958, 651) (2803191, 651)\n",
      "(14015958, 715) (2803191, 715)\n",
      "(14015958, 747) (2803191, 747)\n",
      "(14015958, 779) (2803191, 779)\n",
      "(14015958, 811) (2803191, 811)\n",
      "(14015958, 843) (2803191, 843)\n",
      "done 6!\n",
      "726 726\n",
      "training\n",
      "train done!\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fc6597834922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mtitile_kfold_static\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'like'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mgbdt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'finish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mgbdt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'like'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6bfdc758bf50>\u001b[0m in \u001b[0;36mgbdt\u001b[0;34m(train_df, test_df, flag, label)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mout_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0mout_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_G'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trees\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mout_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/xgboost-0.81-py3.6.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, X, ntree_limit)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgaps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumbering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \"\"\"\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mtest_dmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         return self.get_booster().predict(test_dmatrix,\n\u001b[1;32m    460\u001b[0m                                           \u001b[0mpred_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/xgboost-0.81-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    382\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    383\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/xgboost-0.81-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mfeature_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPANDAS_DTYPE_MAPPER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dtypes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path1,path2,flag in [('data/train_dev.pkl','data/dev.pkl','dev'),('data/train.pkl','data/test.pkl','test')]:\n",
    "        print(path1,path2)\n",
    "        train_df=pd.read_pickle(path1)\n",
    "        test_df=pd.read_pickle(path2) \n",
    "        #train_df['ids']=list(range(len(train_df)))\n",
    "        #test_df['ids']=list(range(len(test_df)))\n",
    "        #w2v(train_df,test_df,'item_id',flag)\n",
    "        print(train_df.shape,test_df.shape)\n",
    "        #last_did(train_df,test_df)\n",
    "        #trajectary(train_df,test_df,'uid','item_id')\n",
    "        #trajectary(train_df,test_df,'did','author_id')\n",
    "        \"\"\"\n",
    "        deepwalk(train_df,test_df,'uid','item_id',flag,128)\n",
    "        w2v(train_df,test_df,'author_id',flag,128)\n",
    "        w2v(train_df,test_df,'item_id',flag,128)\n",
    "        w2v_1(train_df,test_df,'uid',flag,128)\n",
    "        w2v_1(train_df,test_df,'did',flag,128)\n",
    "        last_author_id(train_df,test_df)\n",
    "        last_item_id(train_df,test_df)\n",
    "\n",
    "        for f in ['item_id','author_id','did']:\n",
    "            kfold_static(train_df,test_df,f,'finish') \n",
    "        for f in ['item_id','author_id','did','uid','user_city','item_city','music_id']:\n",
    "            kfold_static(train_df,test_df,f,'like')    \n",
    "        for f in ['author_id']:\n",
    "            w2v(train_df,test_df,f,flag)\n",
    "        for f in ['uid','did']:\n",
    "            w2v_1(train_df,test_df,'uid',flag)\n",
    "            w2v_1(train_df,test_df,'did',flag)\n",
    "        \n",
    "        combine(train_df,test_df,'author_id','did')\n",
    "        for f in ['did','author_id','item_id','author_id_did']:\n",
    "            all_cont(train_df,test_df,f)\n",
    "            day_cont(train_df,test_df,f)\n",
    "\n",
    "        var_mean(train_df,test_df)\n",
    "        author_features(train_df,test_df)\n",
    "        did_features(train_df,test_df) \n",
    "        kfold_static(train_df,test_df,'last_item_id','finish')\n",
    "        kfold_static(train_df,test_df,'last_author','finish')\n",
    "        \"\"\"\n",
    "        #uid_item_time_stamp(train_df,test_df)\n",
    "        #deepwalk(train_df,test_df,'uid','item_id',flag,64)\n",
    "        #deepwalk(train_df,test_df,'uid','author_id',flag,64)\n",
    "        #deepwalk(train_df,test_df,'did','item_id',flag,64)\n",
    "        #deepwalk(train_df,test_df,'did','author_id',flag,64)\n",
    "        \"\"\"\n",
    "        #================================================================================\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"item_id\",\"unique_mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"author_id\",\"unique_mean\")  \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"channel\",\"unique_mean\") \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"item_id\",\"unique_mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"author_id\",\"unique_mean\")  \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"channel\",\"unique_mean\")\n",
    "        #================================================================================\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"item_id\",\"unique_var\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"author_id\",\"unique_var\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"channel\",\"unique_var\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"item_id\",\"unique_var\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"author_id\",\"unique_var\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"channel\",\"unique_var\")\n",
    "        #================================================================================\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"author_id\",\"title_cont\",\"skew\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"author_id\",\"title_cont\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"author_id\",\"title_cont\",\"std\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"title_cont\",\"skew\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"title_cont\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"title_cont\",\"std\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,[\"uid\",\"channel\"],\"title_cont\",\"skew\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,[\"uid\",\"channel\"],\"title_cont\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,[\"uid\",\"channel\"],\"title_cont\",\"std\")\n",
    "        #================================================================================ \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"did\",\"nunique\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"did\",\"count\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"channel\",\"nunique\")\n",
    "        #================================================================================\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"video_duration\",\"min\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"video_duration\",\"max\") \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"video_duration\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"video_duration\",\"std\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"channel\",\"video_duration\",\"min\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"channel\",\"video_duration\",\"max\") \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"channel\",\"video_duration\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"channel\",\"video_duration\",\"std\")\n",
    "        #================================================================================\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"item_id\",\"uid\",\"nunique\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"item_id\",\"uid\",\"count\")        \n",
    "        #================================================================================\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"author_id\",\"item_id\",\"nunique\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"author_id\",\"item_id\",\"count\")        \n",
    "        #================================================================================  \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"user_city\",\"nunique\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"author_id\",\"nunique\")        \n",
    "        #================================================================================          \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"channel\",\"user_city\",\"nunique\")          \n",
    "        #================================================================================         \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"video_duration\",\"skew\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"channel\",\"video_duration\",\"skew\")        \n",
    "        #================================================================================\n",
    "        pagerank(train_df,test_df,'uid','item_id',0.8,100)\n",
    "        pagerank(train_df,test_df,'uid','author_id',0.8,100)\n",
    "        pagerank(train_df,test_df,'did','item_id',0.8,100)\n",
    "        pagerank(train_df,test_df,'did','author_id',0.8,100)\n",
    "        pagerank(train_df,test_df,'item_id','uid',0.8,100)\n",
    "        pagerank(train_df,test_df,'item_id','did',0.8,100)\n",
    "        pagerank(train_df,test_df,'author_id','uid',0.8,100)\n",
    "        pagerank(train_df,test_df,'author_id','did',0.8,100)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        title_mean(train_df,test_df)\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"title_mean\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"title_mean\",\"std\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"uid\",\"title_mean\",\"skew\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"author_id\",\"title_mean\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"author_id\",\"title_mean\",\"std\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"author_id\",\"title_mean\",\"skew\")  \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"title_mean\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"title_mean\",\"std\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,\"did\",\"title_mean\",\"skew\") \n",
    "        train_df,test_df=get_agg_features(train_df,test_df,[\"uid\",\"channel\"],\"title_mean\",\"mean\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,[\"uid\",\"channel\"],\"title_mean\",\"std\")\n",
    "        train_df,test_df=get_agg_features(train_df,test_df,[\"uid\",\"channel\"],\"title_mean\",\"skew\") \n",
    "        titile_kfold_static(train_df,test_df,'finish')\n",
    "        titile_kfold_static(train_df,test_df,'like')\n",
    "        \"\"\"\n",
    "        gbdt(train_df,test_df,flag,'finish')\n",
    "        gbdt(train_df,test_df,flag,'like')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "        print(list(train_df))\n",
    "        #train_df.to_pickle(path1) \n",
    "        #test_df.to_pickle(path2)  \n",
    "        print(\"*\"*80)\n",
    "        print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_dev.pkl data/dev.pkl\n",
      "(14015958, 203) (2803191, 203)\n",
      "(14015958, 235) (2803191, 235)\n",
      "(14015958, 267) (2803191, 267)\n",
      "(14015958, 299) (2803191, 299)\n",
      "done 1!\n",
      "(14015958, 315) (2803191, 315)\n",
      "(14015958, 331) (2803191, 331)\n",
      "(14015958, 347) (2803191, 347)\n",
      "(14015958, 363) (2803191, 363)\n",
      "done 2!\n",
      "(14015958, 379) (2803191, 379)\n",
      "(14015958, 395) (2803191, 395)\n",
      "(14015958, 411) (2803191, 411)\n",
      "(14015958, 427) (2803191, 427)\n",
      "done 3!\n",
      "(14015958, 443) (2803191, 443)\n",
      "(14015958, 459) (2803191, 459)\n",
      "(14015958, 475) (2803191, 475)\n",
      "(14015958, 491) (2803191, 491)\n",
      "done 4!\n",
      "(14015958, 507) (2803191, 507)\n",
      "(14015958, 523) (2803191, 523)\n",
      "(14015958, 539) (2803191, 539)\n",
      "(14015958, 555) (2803191, 555)\n",
      "(14015958, 571) (2803191, 571)\n",
      "(14015958, 587) (2803191, 587)\n",
      "(14015958, 603) (2803191, 603)\n",
      "(14015958, 619) (2803191, 619)\n",
      "done 5!\n",
      "(14015958, 651) (2803191, 651)\n",
      "(14015958, 715) (2803191, 715)\n",
      "(14015958, 747) (2803191, 747)\n",
      "(14015958, 779) (2803191, 779)\n",
      "(14015958, 811) (2803191, 811)\n",
      "done 6!\n",
      "694 694\n",
      "training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trian_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c39493cb385a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxgb_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mtrian_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trian_y' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn.decomposition as sk_decomposition\n",
    "import pandas as pd\n",
    "for path1,path2,flag in [('data/train.pkl','data/test.pkl','test')]:\n",
    "    print(path1,path2)\n",
    "    train_df=pd.read_pickle(path1)\n",
    "    test_df=pd.read_pickle(path2) \n",
    "    features=[]\n",
    "    for col in ['author_id','did','music_id','item_id']:\n",
    "        df = pd.read_csv('deepwalk_data_128/user_'+col+'_uid_deepwalk_128.csv')\n",
    "        df = df.drop_duplicates(['uid'])\n",
    "        dim=32\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove('uid')\n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on='uid', how='left')\n",
    "        test_df = pd.merge(test_df, df1, on='uid', how='left')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "    print(\"done 1!\")\n",
    "\n",
    "    for col in ['author_id','did','music_id','uid']:\n",
    "        df = pd.read_pickle('deepwalk_data_64/item_'+col+'_uid_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates(['item_id'])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove('item_id') \n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on='item_id', how='left')\n",
    "        test_df = pd.merge(test_df, df1, on='item_id', how='left')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "    print(\"done 2!\")\n",
    "\n",
    "    for col in ['author_id','music_id','uid','item_id']:\n",
    "        df = pd.read_pickle('deepwalk_data_64/did_'+col+'_did_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates(['did'])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove('did') \n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on='did', how='left')\n",
    "        test_df = pd.merge(test_df, df1, on='did', how='left')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "    print(\"done 3!\")    \n",
    "\n",
    "    for col in ['did','music_id','uid','item_id']:\n",
    "        df = pd.read_pickle('deepwalk_data_64/author_'+col+'_author_id_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates(['author_id'])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove('author_id')  \n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on='author_id', how='left')\n",
    "        test_df = pd.merge(test_df, df1, on='author_id', how='left')\n",
    "        print(train_df.shape,test_df.shape)\n",
    "    print(\"done 4!\")    \n",
    "\n",
    "    for f1,f2 in [('uid','item_id'),('uid','author_id'),('did','item_id'),('did','author_id')]:\n",
    "        col=f1\n",
    "        df = pd.read_pickle( 'data/' +f1+'_'+ f2+'_'+col +'_'+flag +'_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates([col])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove(col)\n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on=col, how='left')\n",
    "        test_df = pd.merge(test_df, df1, on=col, how='left')\n",
    "        print(train_df.shape,test_df.shape) \n",
    "\n",
    "        col=f2\n",
    "        df = pd.read_pickle( 'data/' +f1+'_'+ f2+'_'+col +'_'+flag +'_deepwalk_64.pkl')\n",
    "        df = df.drop_duplicates([col])\n",
    "        dim=16\n",
    "        pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "        pca.fit(df[df.columns[1:]])\n",
    "        df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "        df1.columns=df.columns[1:dim+1]\n",
    "        df1[df.columns[0]]=df[df.columns[0]]\n",
    "        fs = list(df1)\n",
    "        fs.remove(col)\n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df1, on=col, how='left')\n",
    "        test_df = pd.merge(test_df, df1, on=col, how='left')\n",
    "        print(train_df.shape,test_df.shape)                 \n",
    "    print(\"done 5!\")\n",
    "\n",
    "    for col in ['video','audio','author_id','uid','did']:\n",
    "        if col in ['audio']:\n",
    "            df = pd.read_pickle( 'data/' + col + '_w2v_svd_64.pkl')\n",
    "            col='item_id'\n",
    "        elif col in ['video']:\n",
    "            df = pd.read_pickle( 'data/' + col + '_w2v_svd_32.pkl')\n",
    "            col='item_id'    \n",
    "        else:\n",
    "            df = pd.read_pickle( 'data/' + col + '_'+flag+'_w2v_128.pkl')\n",
    "            df = df.drop_duplicates([col])\n",
    "            dim=32\n",
    "            pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "            pca.fit(df[df.columns[1:]])\n",
    "            df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "            df1.columns=df.columns[1:dim+1]\n",
    "            df1[df.columns[0]]=df[df.columns[0]]\n",
    "            df=df1\n",
    "        df = df.drop_duplicates([col])\n",
    "        fs = list(df)\n",
    "        fs.remove(col)\n",
    "        features+=fs\n",
    "        train_df = pd.merge(train_df, df, on=col, how='left')\n",
    "        test_df = pd.merge(test_df, df, on=col, how='left')\n",
    "        print(train_df.shape,test_df.shape) \n",
    "    del df\n",
    "    del df1\n",
    "    gc.collect()\n",
    "    print(\"done 6!\")\n",
    "    finish_features=['uid','user_city','item_id','author_id','item_city','channel','music_id','did','video_duration',\n",
    "                  'did_finish_positive_num', 'did_finish_negative_num', 'did_finish_rate', 'did_all_cont','did_day_cont',\n",
    "                  'author_id_finish_positive_num', 'author_id_finish_negative_num', 'author_id_finish_rate','author_id_all_cont','author_id_day_cont',\n",
    "                  'item_id_finish_positive_num', 'item_id_finish_negative_num', 'item_id_finish_rate','item_id_all_cont','item_id_day_cont',\n",
    "                  'author_id_did_all_cont','author_include_num_of_item','author_include_num_of_uid','uid_num_of_author_mean','uid_num_of_author_var','uid_num_of_author_fft_var',\n",
    "                  'title_cont','beauty_min','uid_has_num_of_did','mean_id','id_mean_differ','max_id','id_max_differ','min_id','id_min_differ',\n",
    "                  'title_mean_rate_finish','title_max_rate_finish','title_min_rate_finish','title_mean',  \n",
    "                  'uid_channel_title_mean_mean', 'uid_channel_title_mean_std', 'uid_channel_title_mean_skew',\n",
    "                   'uid_item_id_pagerank', 'uid_author_id_pagerank', 'did_item_id_pagerank', 'did_author_id_pagerank', 'item_id_uid_pagerank', 'item_id_did_pagerank', 'author_id_uid_pagerank', 'author_id_did_pagerank']\n",
    "    like_features=['uid','user_city','item_id','author_id','item_city','channel','music_id','did','video_duration','did_like_positive_num', 'did_like_negative_num', 'did_like_rate', 'did_all_cont','did_day_cont',\n",
    "                      'author_id_like_positive_num', 'author_id_like_negative_num','author_id_like_rate','author_id_all_cont','author_id_day_cont','item_id_like_positive_num', 'item_id_like_negative_num','item_id_like_rate','item_id_all_cont','item_id_day_cont',\n",
    "                      'uid_like_positive_num', 'uid_like_negative_num', 'uid_like_rate','user_city_like_positive_num', 'user_city_like_negative_num', 'user_city_like_rate','item_city_like_positive_num', 'item_city_like_negative_num', 'item_city_like_rate',\n",
    "                      'music_id_like_positive_num', 'music_id_like_negative_num', 'music_id_like_rate','author_include_num_of_item','author_include_num_of_uid',\n",
    "                      'uid_num_of_author_mean','uid_num_of_author_var','uid_num_of_author_fft_var','author_id_did_all_cont','title_cont','beauty_min','uid_has_num_of_did',\n",
    "                      'mean_id','id_mean_differ','max_id','id_max_differ','min_id','id_min_differ','title_mean_rate_like','uid_item_id_unique_mean', 'uid_author_id_unique_mean', \n",
    "                      'uid_channel_unique_mean', 'did_item_id_unique_mean', 'did_author_id_unique_mean', 'did_channel_unique_mean', 'uid_item_id_unique_var', 'uid_author_id_unique_var', 'uid_channel_unique_var', 'did_item_id_unique_var', \n",
    "                     'did_author_id_unique_var', 'did_channel_unique_var', 'title_mean', 'uid_title_mean_mean', 'uid_title_mean_std', 'uid_title_mean_skew', 'author_id_title_mean_mean', \n",
    "                     'author_id_title_mean_std', 'author_id_title_mean_skew', 'did_title_mean_mean', 'did_title_mean_std', 'did_title_mean_skew', 'uid_channel_title_mean_mean', 'uid_channel_title_mean_std', 'uid_channel_title_mean_skew',\n",
    "                     'uid_item_id_pagerank', 'uid_author_id_pagerank', 'did_item_id_pagerank', 'did_author_id_pagerank', 'item_id_uid_pagerank', 'item_id_did_pagerank', 'author_id_uid_pagerank', 'author_id_did_pagerank']\n",
    "    \n",
    "    for label in ['finish']:\n",
    "        if label=='finish':\n",
    "            xgb_features=features+finish_features\n",
    "        else:\n",
    "            xgb_features=features+like_features\n",
    "        print(len(xgb_features),len(set(xgb_features)))\n",
    "        n_trees=30\n",
    "        print(\"training\")\n",
    "        clf = xgb.XGBClassifier(learning_rate=1, n_estimators=n_trees, max_depth=8,objective='binary:logistic',seed=0,nthread=-1)\n",
    "        clf.fit(train_df[xgb_features], train_df[label])\n",
    "        gc.collect()\n",
    "        print('train done!')\n",
    "        out_df=[]\n",
    "        for i in range(10):\n",
    "            out_df.append(pd.DataFrame(clf.apply(train_df.iloc[int(i/10*len(train_df)):int((i+1)/10*len(train_df))][xgb_features]))) \n",
    "        out_df=pd.concat(out_df,0)\n",
    "        out_df.columns = [label+'_G' + str(i) for i in range(n_trees )]\n",
    "        out_df.to_pickle(\"data/gbdt_train_\"+label+'_'+flag+'.pkl')\n",
    "        print(\"convert train done!\")\n",
    "        del out_df\n",
    "        gc.collect()\n",
    "        out_df = pd.DataFrame(clf.apply(test_df[xgb_features]))\n",
    "        out_df.columns = [label+'_G' + str(i) for i in range(n_trees )]\n",
    "        out_df.to_pickle(\"data/gbdt_test_\"+label+'_'+flag+'.pkl')\n",
    "        del out_df\n",
    "        gc.collect()\n",
    "        print(\"convert test done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate a non-NDFrame object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6858f10c4c5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mout_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxgb_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mout_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mout_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_G'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trees\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mout_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m   1449\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m   1452\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot concatenate a non-NDFrame object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate a non-NDFrame object"
     ]
    }
   ],
   "source": [
    "del train_y\n",
    "del train_x\n",
    "gc.collect()\n",
    "out_df=[]\n",
    "for i in range(10):\n",
    "    out_df.append(clf.apply(train_df.iloc[int(i/10*len(train_df)):int((i+1)/10*len(train_df))][xgb_features])) \n",
    "out_df=pd.concat(out_df,0)\n",
    "out_df.columns = [label+'_G' + str(i) for i in range(n_trees )]\n",
    "out_df['ids']=train_df['ids']\n",
    "out_df.to_pickle(\"data/gbdt_train_\"+label+'_'+flag+'.pkl')\n",
    "print(\"convert train done!\")\n",
    "del out_df\n",
    "gc.collect()\n",
    "out_df = pd.DataFrame(clf.apply(test_df[xgb_features]))\n",
    "out_df.columns = [label+'_G' + str(i) for i in range(n_trees )]\n",
    "out_df['ids']=test_df['ids']\n",
    "out_df.to_pickle(\"data/gbdt_test_\"+label+'_'+flag+'.pkl')\n",
    "del out_df\n",
    "gc.collect()\n",
    "print(\"convert test done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert train done!\n",
      "convert test done!\n"
     ]
    }
   ],
   "source": [
    "out_df=[pd.DataFrame(x) for x in out_df] \n",
    "out_df=pd.concat(out_df,0)\n",
    "out_df.columns = [label+'_G' + str(i) for i in range(n_trees )]\n",
    "out_df['ids']=train_df['ids']\n",
    "out_df.to_pickle(\"data/gbdt_train_\"+label+'_'+flag+'.pkl')\n",
    "print(\"convert train done!\")\n",
    "del out_df\n",
    "gc.collect()\n",
    "out_df = pd.DataFrame(clf.apply(test_df[xgb_features]))\n",
    "out_df.columns = [label+'_G' + str(i) for i in range(n_trees )]\n",
    "out_df['ids']=test_df['ids']\n",
    "out_df.to_pickle(\"data/gbdt_test_\"+label+'_'+flag+'.pkl')\n",
    "del out_df\n",
    "gc.collect()\n",
    "print(\"convert test done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bucket(data,num=10):\n",
    "    data.sort()\n",
    "    bins=[data[0]]\n",
    "    last_idx=0\n",
    "    for idx,t in enumerate(data[1:]):\n",
    "        if t!=bins[-1] and idx-last_idx>=(len(data)//num):\n",
    "            last_idx=idx\n",
    "            bins.append(t)\n",
    "    return bins    \n",
    "\n",
    "def norm(train_df,test_df,features):   \n",
    "    df=pd.concat([train_df,test_df])[features]\n",
    "    scaler = preprocessing.QuantileTransformer(random_state=0)\n",
    "    scaler.fit(df[features]) \n",
    "    train_df[features]=scaler.transform(train_df[features])\n",
    "    test_df[features]=scaler.transform(test_df[features])\n",
    "\n",
    "\n",
    "    \n",
    "for path1,path2,flag in [('data/train_dev.pkl','data/dev.pkl','dev'),('data/train.pkl','data/test.pkl','test')]:\n",
    "        print(path1,path2)\n",
    "        train_df=pd.read_pickle(path1)\n",
    "        test_df=pd.read_pickle(path2)\n",
    "        print(train_df.shape,test_df.shape)\n",
    "        float_features=['uid_did_nunique', 'uid_did_count', 'uid_channel_nunique', 'did_video_duration_min', \n",
    "                        'did_video_duration_max', 'did_video_duration_mean', 'did_video_duration_std', \n",
    "                        'channel_video_duration_min', 'channel_video_duration_max', 'channel_video_duration_mean', \n",
    "                        'channel_video_duration_std', 'uid_item_id_unique_mean', 'uid_author_id_unique_mean', \n",
    "                        'uid_channel_unique_mean', 'did_item_id_unique_mean', 'did_author_id_unique_mean', \n",
    "                        'did_channel_unique_mean', 'uid_item_id_unique_var', 'uid_author_id_unique_var', \n",
    "                        'uid_channel_unique_var', 'did_item_id_unique_var', 'did_author_id_unique_var', \n",
    "                        'did_channel_unique_var', 'author_id_title_cont_skew', 'author_id_title_cont_mean', \n",
    "                        'author_id_title_cont_std', 'did_title_cont_skew', 'did_title_cont_mean', \n",
    "                        'did_title_cont_std', 'uid_channel_title_cont_skew', 'uid_channel_title_cont_mean', \n",
    "                        'uid_channel_title_cont_std', 'item_id_uid_nunique', 'item_id_uid_count', \n",
    "                        'author_id_item_id_nunique', 'author_id_item_id_count', 'uid_user_city_nunique', \n",
    "                        'uid_author_id_nunique', 'channel_user_city_nunique', 'did_video_duration_skew', \n",
    "                        'channel_video_duration_skew',  'item_id_pagerank', 'uid_item_id_pagerank', \n",
    "                        'uid_author_id_pagerank', 'did_item_id_pagerank', 'did_author_id_pagerank',\n",
    "                        'item_id_uid_pagerank', 'item_id_did_pagerank', 'author_id_uid_pagerank', \n",
    "                        'author_id_did_pagerank', 'title_mean', 'uid_title_mean_mean', \n",
    "                        'uid_title_mean_std', 'uid_title_mean_skew', 'author_id_title_mean_mean', \n",
    "                        'author_id_title_mean_std', 'author_id_title_mean_skew', 'did_title_mean_mean', \n",
    "                        'did_title_mean_std', 'did_title_mean_skew', 'uid_channel_title_mean_mean', \n",
    "                        'uid_channel_title_mean_std', 'uid_channel_title_mean_skew',\n",
    "                        'uid_num_of_author_mean','uid_num_of_author_var','uid_num_of_author_fft_var',]\n",
    "        train_df=train_df.fillna(-1)\n",
    "        test_df=test_df.fillna(-1)\n",
    "        norm(train_df,test_df,float_features)\n",
    "        print(train_df[float_features])\n",
    "        \n",
    "        k=10\n",
    "        train_df=train_df.sample(frac=1)\n",
    "        train=[(path2[:-4]+'_NN.pkl',test_df)]\n",
    "        for i in range(k):\n",
    "            train.append((path1[:-4]+'_NN_'+str(i)+'.pkl',train_df.iloc[int(i/k*len(train_df)):int((i+1)/k*len(train_df))]))\n",
    "        del train_df\n",
    "        gc.collect()\n",
    "        for file,temp in train:\n",
    "            print(file,temp.shape)\n",
    "            for col in ['author_id','did','music_id','item_id']:\n",
    "                df = pd.read_csv('deepwalk_data_128/user_'+col+'_uid_deepwalk_128.csv')\n",
    "                df = df.drop_duplicates(['uid'])\n",
    "                fs = list(df)\n",
    "                fs.remove('uid')    \n",
    "                temp = pd.merge(temp, df, on='uid', how='left')\n",
    "                print(temp.shape)\n",
    "            print(\"done 1!\")\n",
    "            for col in ['author_id','did','music_id','uid']:\n",
    "                df = pd.read_pickle('deepwalk_data_64/item_'+col+'_uid_deepwalk_64.pkl')\n",
    "                df = df.drop_duplicates(['item_id'])\n",
    "                fs = list(df)\n",
    "                fs.remove('item_id')    \n",
    "                temp = pd.merge(temp, df, on='item_id', how='left')\n",
    "                print(temp.shape)\n",
    "            print(\"done 2!\")\n",
    "            for col in ['author_id','music_id','uid','item_id']:\n",
    "                df = pd.read_pickle('deepwalk_data_64/did_'+col+'_did_deepwalk_64.pkl')\n",
    "                df = df.drop_duplicates(['did'])\n",
    "                fs = list(df)\n",
    "                fs.remove('did')    \n",
    "                temp = pd.merge(temp, df, on='did', how='left')\n",
    "                print(temp.shape) \n",
    "            print(\"done 3!\")    \n",
    "            for col in ['did','music_id','uid','item_id']:\n",
    "                df = pd.read_pickle('deepwalk_data_64/author_'+col+'_author_id_deepwalk_64.pkl')\n",
    "                df = df.drop_duplicates(['author_id'])\n",
    "                fs = list(df)\n",
    "                fs.remove('author_id')    \n",
    "                temp = pd.merge(temp, df, on='author_id', how='left')\n",
    "                print(temp.shape) \n",
    "            print(\"done 4!\")    \n",
    "  \n",
    "            for f1,f2 in [('uid','item_id'),('uid','author_id'),('did','item_id'),('did','author_id')]:\n",
    "                col=f1\n",
    "                df = pd.read_pickle( 'data/' +f1+'_'+ f2+'_'+col +'_'+flag +'_deepwalk_64.pkl')\n",
    "                df = df.drop_duplicates([col])\n",
    "                fs = list(df)\n",
    "                fs.remove(col)\n",
    "                temp = pd.merge(temp, df, on=col, how='left')\n",
    "                print(temp.shape) \n",
    "                col=f2\n",
    "                df = pd.read_pickle( 'data/' +f1+'_'+ f2+'_'+col +'_'+flag +'_deepwalk_64.pkl')\n",
    "                df = df.drop_duplicates([col])\n",
    "                fs = list(df)\n",
    "                fs.remove(col)\n",
    "                temp = pd.merge(temp, df, on=col, how='left')\n",
    "                print(temp.shape)                 \n",
    "            print(\"done 5!\")\n",
    "            \n",
    "            for col in ['video','audio','author_id','uid','did','item_id']:\n",
    "                if col in ['audio']:\n",
    "                    df = pd.read_pickle( 'data/' + col + '_w2v_svd_64.pkl')\n",
    "                    col='item_id'\n",
    "                elif col in ['video']:\n",
    "                    df = pd.read_pickle( 'data/' + col + '_w2v_svd_32.pkl')\n",
    "                    col='item_id'    \n",
    "                else:\n",
    "                    df = pd.read_pickle( 'data/' + col + '_'+flag+'_w2v_128.pkl')\n",
    "                df = df.drop_duplicates([col])\n",
    "                fs = list(df)\n",
    "                fs.remove(col)    \n",
    "                print(temp.shape)\n",
    "                temp = pd.merge(temp, df, on=col, how='left')\n",
    "                print(temp.shape) \n",
    "            print(\"done 6!\")\n",
    "            temp=temp.fillna(0)           \n",
    "            temp.to_pickle(file)\n",
    "            del temp\n",
    "            gc.collect()\n",
    "        \n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_dev.pkl data/dev.pkl\n",
      "(14015958, 171) (2803191, 171)\n",
      "data/dev_xgb.pkl (2803191, 171)\n",
      "(2803191, 203)\n",
      "done 1!\n",
      "done 2!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ef1c256b834f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'did'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'did'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done 3!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                          copy=copy, indicator=indicator)\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmerge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_doc\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'\\nleft : DataFrame'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m                 self.left, self.right)\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             (left_indexer,\n\u001b[0;32m--> 714\u001b[0;31m              right_indexer) = self._get_join_indexers()\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m                                   \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m                                   how=self.how)\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[0;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;31m# get flat i8 keys from label lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m     \u001b[0mlkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_join_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[0;31m# factorize keys to a dense i8 space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/env3/lib/python3.6/site-packages/pandas/tools/merge.py\u001b[0m in \u001b[0;36m_get_join_keys\u001b[0;34m(llab, rlab, shape, sort)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;31m# get keys for the first `nlev` levels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnlev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m     \u001b[0mlkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mllab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m     \u001b[0mrkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrlab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn.decomposition as sk_decomposition\n",
    "import pandas as pd\n",
    "def make_bucket(data,num=10):\n",
    "    data.sort()\n",
    "    bins=[data[0]]\n",
    "    last_idx=0\n",
    "    for idx,t in enumerate(data[1:]):\n",
    "        if t!=bins[-1] and idx-last_idx>=(len(data)//num):\n",
    "            last_idx=idx\n",
    "            bins.append(t)\n",
    "    return bins    \n",
    "\n",
    "def norm(train_df,test_df,features):   \n",
    "    df=pd.concat([train_df,test_df])[features]\n",
    "    scaler = preprocessing.QuantileTransformer(random_state=0)\n",
    "    scaler.fit(df[features]) \n",
    "    train_df[features]=scaler.transform(train_df[features])\n",
    "    test_df[features]=scaler.transform(test_df[features])\n",
    "\n",
    "\n",
    "    \n",
    "for path1,path2,flag in [('data/train_dev.pkl','data/dev.pkl','dev'),('data/train.pkl','data/test.pkl','test')]:\n",
    "        print(path1,path2)\n",
    "        train_df=pd.read_pickle(path1)\n",
    "        test_df=pd.read_pickle(path2)\n",
    "        print(train_df.shape,test_df.shape)\n",
    "\n",
    "        k=1\n",
    "        svd_dic={}\n",
    "        train_df=train_df.sample(frac=1)\n",
    "        train=[(path2[:-4]+'_xgb.pkl',test_df)]\n",
    "        for i in range(k):\n",
    "            train.append((path1[:-4]+'_xgb_'+str(i)+'.pkl',train_df.iloc[int(i/k*len(train_df)):int((i+1)/k*len(train_df))]))\n",
    "        del train_df\n",
    "        gc.collect()\n",
    "        for file,temp in train:\n",
    "            print(file,temp.shape)\n",
    "            for col in ['item_id']:\n",
    "                if 'deepwalk_data_128/user_'+col+'_uid_deepwalk_128.csv' in svd_dic:\n",
    "                    df=svd_dic['deepwalk_data_128/user_'+col+'_uid_deepwalk_128.csv']\n",
    "                else:\n",
    "                    df = pd.read_csv('deepwalk_data_128/user_'+col+'_uid_deepwalk_128.csv')\n",
    "                    dim=32\n",
    "                    pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "                    pca.fit(df[df.columns[1:]])\n",
    "                    df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "                    df1.columns=df.columns[1:dim+1]\n",
    "                    df1[df.columns[0]]=df[df.columns[0]]  \n",
    "                    df=df1\n",
    "                    del df1\n",
    "                    gc.collect()\n",
    "                    svd_dic['deepwalk_data_128/user_'+col+'_uid_deepwalk_128.csv']=df\n",
    "                \n",
    "                df = df.drop_duplicates(['uid'])\n",
    "                fs = list(df)\n",
    "                fs.remove('uid')    \n",
    "                temp = pd.merge(temp, df, on='uid', how='left')\n",
    "                print(temp.shape) \n",
    "            print(\"done 1!\")\n",
    "            \n",
    "            \n",
    "            for col in ['uid']:\n",
    "                if 'deepwalk_data_64/item_'+col+'_uid_deepwalk_64.pkl' in svd_dic:\n",
    "                    df=svd_dic['deepwalk_data_64/item_'+col+'_uid_deepwalk_64.pkl']\n",
    "                else:\n",
    "                    df = pd.read_pickle('deepwalk_data_64/item_'+col+'_uid_deepwalk_64.pkl')\n",
    "                    dim=16\n",
    "                    pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "                    pca.fit(df[df.columns[1:]])\n",
    "                    df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "                    df1.columns=df.columns[1:dim+1]\n",
    "                    df1[df.columns[0]]=df[df.columns[0]]  \n",
    "                    df=df1\n",
    "                    del df1\n",
    "                    gc.collect()\n",
    "                    svd_dic['deepwalk_data_64/item_'+col+'_uid_deepwalk_64.pkl']=df\n",
    "                \n",
    "                df = df.drop_duplicates(['item_id'])\n",
    "                fs = list(df)\n",
    "                fs.remove('item_id')    \n",
    "                temp = pd.merge(temp, df, on='item_id', how='left')\n",
    "                print(temp.shape)             \n",
    "            print(\"done 2!\")\n",
    "            \n",
    "            for col in ['author_id']:\n",
    "                if 'deepwalk_data_64/did_'+col+'_did_deepwalk_64.pkl' in svd_dic:\n",
    "                    df=svd_dic['deepwalk_data_64/did_'+col+'_did_deepwalk_64.pkl']\n",
    "                else:\n",
    "                    df = pd.read_pickle('deepwalk_data_64/did_'+col+'_did_deepwalk_64.pkl')\n",
    "                    dim=32\n",
    "                    pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "                    pca.fit(df[df.columns[1:]])\n",
    "                    df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "                    df1.columns=df.columns[1:dim+1]\n",
    "                    df1[df.columns[0]]=df[df.columns[0]]  \n",
    "                    df=df1\n",
    "                    del df1\n",
    "                    gc.collect()\n",
    "                    svd_dic['deepwalk_data_64/did_'+col+'_did_deepwalk_64.pkl']=df\n",
    "\n",
    "                df = df.drop_duplicates(['did'])\n",
    "                fs = list(df)\n",
    "                fs.remove('did')    \n",
    "                temp = pd.merge(temp, df, on='did', how='left')\n",
    "                print(temp.shape)              \n",
    "            print(\"done 3!\")  \n",
    "            \n",
    "            for col in ['did']:\n",
    "                if 'deepwalk_data_64/author_'+col+'_author_id_deepwalk_64.pkl' in svd_dic:\n",
    "                    df=svd_dic['deepwalk_data_64/author_'+col+'_author_id_deepwalk_64.pkl']\n",
    "                else:\n",
    "                    df = pd.read_pickle('deepwalk_data_64/author_'+col+'_author_id_deepwalk_64.pkl')\n",
    "                    dim=20\n",
    "                    pca = sk_decomposition.PCA(n_components=dim,whiten=False,svd_solver='auto')\n",
    "                    pca.fit(df[df.columns[1:]])\n",
    "                    df1=pd.DataFrame(pca.transform(df[df.columns[1:]]))\n",
    "                    df1.columns=df.columns[1:dim+1]\n",
    "                    df1[df.columns[0]]=df[df.columns[0]]  \n",
    "                    df=df1\n",
    "                    del df1\n",
    "                    gc.collect()\n",
    "                    svd_dic['deepwalk_data_64/author_'+col+'_author_id_deepwalk_64.pkl']=df\n",
    "                    \n",
    "                df = df.drop_duplicates(['author_id'])\n",
    "                fs = list(df)\n",
    "                fs.remove('author_id')    \n",
    "                temp = pd.merge(temp, df, on='author_id', how='left')\n",
    "                print(temp.shape) \n",
    "            print(\"done 4!\")    \n",
    "  \n",
    "            for col in ['item_id','uid']:\n",
    "                df = pd.read_pickle( 'data/' + col + '_'+flag+'_deepwalk.pkl')\n",
    "                df = df.drop_duplicates([col])\n",
    "                fs = list(df)\n",
    "                fs.remove(col)    \n",
    "                temp = pd.merge(temp, df, on=col, how='left')\n",
    "                print(temp.shape)\n",
    "                del df\n",
    "                gc.collect()\n",
    "            print(\"done 5!\")\n",
    "            \n",
    "            for col in ['video','audio','author_id','uid','did']:\n",
    "                if col in ['audio']:\n",
    "                    df = pd.read_pickle( 'data/' + col + '_w2v_svd_64.pkl')\n",
    "                    col='item_id'\n",
    "                elif col in ['video']:\n",
    "                    df = pd.read_pickle( 'data/' + col + '_w2v_svd_32.pkl')\n",
    "                    col='item_id'    \n",
    "                else:\n",
    "                    df = pd.read_pickle( 'data/' + col + '_'+flag+'_w2v.pkl')\n",
    "                df = df.drop_duplicates([col])\n",
    "                fs = list(df)\n",
    "                fs.remove(col)    \n",
    "                temp = pd.merge(temp, df, on=col, how='left')\n",
    "                print(temp.shape)\n",
    "                del df\n",
    "                gc.collect()\n",
    "            print(\"done 6!\")       \n",
    "            temp.to_pickle(file)\n",
    "            del temp\n",
    "            gc.collect()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
