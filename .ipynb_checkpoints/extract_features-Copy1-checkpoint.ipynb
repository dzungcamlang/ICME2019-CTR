{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "import scipy.special as special\n",
    "from pandas import DataFrame, Series\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from csv import DictReader\n",
    "from sklearn import preprocessing\n",
    "np.random.seed(2019)\n",
    "random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_cont(train_df,test_df,f):    \n",
    "    \"\"\"\n",
    "    data=train_df.append(test_df)\n",
    "    group=data.groupby(f).size()\n",
    "    train_df[f+'_all_cont']=train_df[f].apply(lambda x:group[x])\n",
    "    test_df[f+'_all_cont']=test_df[f].apply(lambda x:group[x])\n",
    "    \"\"\"\n",
    "    print(\"all cont:\",f)\n",
    "    dic={}\n",
    "    for item in train_df[f].values:\n",
    "        try:\n",
    "            dic[item]+=1\n",
    "        except:\n",
    "            dic[item]=1    \n",
    "    for item in test_df[f].values:\n",
    "        try:\n",
    "            dic[item]+=1\n",
    "        except:\n",
    "            dic[item]=1     \n",
    "    cont=[]\n",
    "    for item in train_df[f].values:\n",
    "        cont.append(dic[item])        \n",
    "    train_df[f+'_all_cont']=cont\n",
    "    print('train','done')\n",
    "    \n",
    "    cont=[]\n",
    "    for item in test_df[f].values:\n",
    "        cont.append(dic[item])        \n",
    "    test_df[f+'_all_cont']=cont\n",
    "    print('test','done')            \n",
    "    print(f+'_all_cont')\n",
    "    print('avg of cont',np.mean(train_df[f+'_all_cont']),np.mean(test_df[f+'_all_cont']))   \n",
    "    \n",
    "def day_cont(train_df,test_df,f):\n",
    "    print(\"day cont:\",f)\n",
    "    dic={}\n",
    "    cont=[]\n",
    "    dics=[]\n",
    "    day=0\n",
    "    for item in train_df[['day',f]].values:\n",
    "        item[0]=int(item[0])\n",
    "        if day!=item[0]:\n",
    "            dics.append(dic)\n",
    "            dic={}\n",
    "            day+=1\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]]+=1\n",
    "        except:\n",
    "            dic[item[1]]=1\n",
    "    dics.append(dic)        \n",
    "    day=0\n",
    "    dic=dics[day]\n",
    "    for item in train_df[['day',f]].values:\n",
    "        item[0]=int(item[0])\n",
    "        if day!=item[0]:\n",
    "            day+=1 \n",
    "            dic=dics[day]\n",
    "            print(day) \n",
    "        cont.append(dic[item[1]])        \n",
    "    train_df[f+'_day_cont']=cont\n",
    "    print('train','done')\n",
    "    \n",
    "    dic={}\n",
    "    for item in test_df[f].values:\n",
    "        try:\n",
    "            dic[item]+=1\n",
    "        except:\n",
    "            dic[item]=1            \n",
    "    cont=[]\n",
    "    for item in test_df[f].values:\n",
    "        cont.append(dic[item])\n",
    "    test_df[f+'_day_cont']=cont    \n",
    "    print('test','done')\n",
    "    print(f+'_day_cont')\n",
    "    print('avg of cont',np.mean(train_df[f+'_day_cont']),np.mean(test_df[f+'_day_cont']))   \n",
    "    \n",
    "def combine(train_df,test_df,f1,f2):\n",
    "    train_df[f1+'_'+f2]=train_df[f1]*1e7+train_df[f2]\n",
    "    train_df[f1+'_'+f2]=train_df[f1+'_'+f2].astype(int)\n",
    "    test_df[f1+'_'+f2]=test_df[f1]*1e7 +test_df[f2]\n",
    "    test_df[f1+'_'+f2]=test_df[f1+'_'+f2].astype(int)\n",
    "    \n",
    "def kfold_static(train_df,test_df,f,label):\n",
    "    print(\"K-fold static:\",f+'_'+label)\n",
    "    #K-fold positive and negative num\n",
    "    avg_rate=train_df[label].mean()\n",
    "    num=len(train_df)//5\n",
    "    index=[0 for i in range(num)]+[1 for i in range(num)]+[2 for i in range(num)]+[3 for i in range(num)]+[4 for i in range(len(train_df)-4*num)]\n",
    "    random.shuffle(index)\n",
    "    train_df['index']=index\n",
    "\n",
    "        \n",
    "\n",
    "    dic=[{} for i in range(5)]\n",
    "    dic_all={}\n",
    "    for item in train_df[['index',f,label]].values:\n",
    "        try:\n",
    "            dic[item[0]][item[1]][item[2]]+=1\n",
    "        except:\n",
    "            dic[item[0]][item[1]]=[0,0]\n",
    "            dic[item[0]][item[1]][item[2]]+=1\n",
    "        try:\n",
    "            dic_all[item[1]][item[2]]+=1\n",
    "        except:\n",
    "            dic_all[item[1]]=[0,0]\n",
    "            dic_all[item[1]][item[2]]+=1\n",
    "    print(\"static done!\")\n",
    "                \n",
    "    positive=[]\n",
    "    negative=[]\n",
    "    rate=[]\n",
    "    for item in train_df[['index',f]].values:\n",
    "        n,p=dic_all[item[1]]\n",
    "        try:\n",
    "            p-=dic[item[0]][item[1]][1]\n",
    "            n-=dic[item[0]][item[1]][0] \n",
    "        except:\n",
    "            pass\n",
    "        if p==0 and n==0:\n",
    "            positive.append(-1)\n",
    "            negative.append(-1)\n",
    "            rate.append(avg_rate)\n",
    "        else:\n",
    "            positive.append(p)\n",
    "            negative.append(n)\n",
    "            rate.append(p/(p+n))  \n",
    "            \n",
    "    train_df[f+'_'+label+'_positive_num']=positive\n",
    "    train_df[f+'_'+label+'_negative_num']=negative\n",
    "    train_df[f+'_'+label+'_rate']=rate\n",
    "    print(\"train done!\")\n",
    "    #for test\n",
    "    positive=[]\n",
    "    negative=[]\n",
    "    rate=[]\n",
    "    for uid in test_df[f].values:\n",
    "        p=0\n",
    "        n=0\n",
    "        try:\n",
    "            p=dic_all[uid][1]\n",
    "            n=dic_all[uid][0]\n",
    "        except:\n",
    "            pass\n",
    "        if p==0 and n==0:\n",
    "            positive.append(-1)\n",
    "            negative.append(-1)\n",
    "            rate.append(avg_rate)\n",
    "        else:\n",
    "            positive.append(p)\n",
    "            negative.append(n)\n",
    "            rate.append(p/(p+n))            \n",
    "        \n",
    "    test_df[f+'_'+label+'_positive_num']=positive\n",
    "    test_df[f+'_'+label+'_negative_num']=negative  \n",
    "    test_df[f+'_'+label+'_rate']=rate\n",
    "    print(\"test done!\")\n",
    "    del train_df['index']\n",
    "    print(f+'_'+label+'_positive_num')\n",
    "    print(f+'_'+label+'_negative_num')\n",
    "    print(f+'_'+label+'_rate')\n",
    "    print('avg of positive num',np.mean(train_df[f+'_'+label+'_positive_num']),np.mean(test_df[f+'_'+label+'_positive_num']))\n",
    "    print('avg of negative num',np.mean(train_df[f+'_'+label+'_negative_num']),np.mean(test_df[f+'_'+label+'_negative_num']))\n",
    "    print('avg of rate',np.mean(train_df[f+'_'+label+'_rate']),np.mean(test_df[f+'_'+label+'_rate']))\n",
    "\n",
    "    \n",
    "    \n",
    "def w2v(train_df,test_df,f,flag):\n",
    "    print(\"w2v:\",f)\n",
    "    L=10\n",
    "    sentence=[]\n",
    "    dic={}\n",
    "    day=0\n",
    "    for item in train_df[['day','uid',f]].values:\n",
    "        if day!=item[0]:\n",
    "            for key in dic:\n",
    "                sentence.append(dic[key])\n",
    "            dic={}\n",
    "            day=item[0]\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]].append(str(item[2]))\n",
    "        except:\n",
    "            dic[item[1]]=[str(item[2])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    dic={}       \n",
    "    for item in test_df[['uid',f]].values:\n",
    "        try:\n",
    "            dic[item[0]].append(str(item[1]))\n",
    "        except:\n",
    "            dic[item[0]]=[str(item[1])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    print(len(sentence))\n",
    "    print('training...')\n",
    "    random.shuffle(sentence)\n",
    "    if f=='item_id':\n",
    "        model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=50)\n",
    "    else:\n",
    "        model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=10)\n",
    "    print('outputing...')\n",
    "\n",
    "\n",
    "    values=set(train_df[f].values)|set(test_df[f].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model[str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f]\n",
    "    for i in range(L):\n",
    "        names.append(names[0]+'_embedding_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' + f +'_'+flag +'_w2v.pkl') \n",
    "    \n",
    "def w2v_1(train_df,test_df,f,flag):\n",
    "    print(\"w2v:\",f)\n",
    "    L=10\n",
    "    sentence=[]\n",
    "    dic={}\n",
    "    day=0\n",
    "    for item in train_df[['day','author_id',f]].values:\n",
    "        if day!=item[0]:\n",
    "            for key in dic:\n",
    "                sentence.append(dic[key])\n",
    "            dic={}\n",
    "            day=item[0]\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]].append(str(item[2]))\n",
    "        except:\n",
    "            dic[item[1]]=[str(item[2])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    dic={}       \n",
    "    for item in test_df[['author_id',f]].values:\n",
    "        try:\n",
    "            dic[item[0]].append(str(item[1]))\n",
    "        except:\n",
    "            dic[item[0]]=[str(item[1])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    print(len(sentence))\n",
    "    print('training...')\n",
    "    random.shuffle(sentence)\n",
    "    model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=10)\n",
    "    print('outputing...')\n",
    "\n",
    "\n",
    "    values=set(train_df[f].values)|set(test_df[f].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model[str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f]\n",
    "    for i in range(L):\n",
    "        names.append(names[0]+'_embedding_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' + f +'_'+flag +'_w2v.pkl') \n",
    "    \n",
    "def w2v_2(train_df,test_df,f,flag):\n",
    "    print(\"w2v:\",f)\n",
    "    L=10\n",
    "    sentence=[]\n",
    "    dic={}\n",
    "    day=0\n",
    "    for item in train_df[['day','item_id',f]].values:\n",
    "        if day!=item[0]:\n",
    "            for key in dic:\n",
    "                sentence.append(dic[key])\n",
    "            dic={}\n",
    "            day=item[0]\n",
    "            print(day)\n",
    "        try:\n",
    "            dic[item[1]].append(str(item[2]))\n",
    "        except:\n",
    "            dic[item[1]]=[str(item[2])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    dic={}       \n",
    "    for item in test_df[['item_id',f]].values:\n",
    "        try:\n",
    "            dic[item[0]].append(str(item[1]))\n",
    "        except:\n",
    "            dic[item[0]]=[str(item[1])]\n",
    "    for key in dic:\n",
    "        sentence.append(dic[key])\n",
    "    print(len(sentence))\n",
    "    print('training...')\n",
    "    random.shuffle(sentence)\n",
    "    model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=10)\n",
    "    print('outputing...')\n",
    "\n",
    "\n",
    "    values=set(train_df[f].values)|set(test_df[f].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model[str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f]\n",
    "    for i in range(L):\n",
    "        names.append(names[0]+'_item_id_embedding_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/item_id_' + f +'_'+flag +'_w2v.pkl') \n",
    "    \n",
    "  \n",
    "    \n",
    "def title_w2v(train_df,test_df):\n",
    "    L=10\n",
    "    sentence=[]\n",
    "    for item in train_df['title_keys'].values:\n",
    "        sentence.append(item.split())\n",
    "    for item in test_df['title_keys'].values:\n",
    "        sentence.append(item.split())    \n",
    "    print(len(sentence))\n",
    "    print('training...')\n",
    "    random.shuffle(sentence)\n",
    "    model = Word2Vec(sentence, size=L, window=10, min_count=1, workers=10,iter=10)\n",
    "    print('outputing...')\n",
    "    \n",
    "    w2v=[]\n",
    "    for item in train_df['title_keys'].values:\n",
    "        array=np.zeros(L)\n",
    "        words=item.split()\n",
    "        size=len(words)\n",
    "        for word in words:\n",
    "            array+=model[word]/size\n",
    "        w2v.append(array)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=['word_embedding_'+str(i) for i in range(L)]\n",
    "    out_df.columns = names\n",
    "\n",
    "    for n in names:\n",
    "        train_df[n]=list(out_df[n].values)\n",
    "    \n",
    "    \n",
    "    w2v=[]\n",
    "    for item in test_df['title_keys'].values:\n",
    "        array=np.zeros(L)\n",
    "        words=item.split()\n",
    "        size=len(words)\n",
    "        for word in words:\n",
    "            array+=model[word]/size\n",
    "        w2v.append(array)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=['word_embedding_'+str(i) for i in range(L)]\n",
    "    out_df.columns = names\n",
    "\n",
    "    for n in names:\n",
    "        test_df[n]=list(out_df[n].values)\n",
    "    print(train_df)\n",
    "    print(test_df)\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "def var_mean(train_df,test_df):\n",
    "    data=train_df[['uid','author_id']].append(test_df[['uid','author_id']])\n",
    "    group=data[['uid','author_id']].groupby('uid')\n",
    "    group=group.apply(lambda x:np.var(np.fft.fft(list(Counter(list(x['author_id'])).values()))))\n",
    "    train_df['uid_num_of_author_fft_var']=train_df['uid'].apply(lambda x:group[x])\n",
    "    test_df['uid_num_of_author_fft_var']=test_df['uid'].apply(lambda x:group[x])\n",
    "    print(train_df[train_df['finish']==1]['uid_num_of_author_fft_var'].mean())\n",
    "    print(train_df[train_df['finish']==0]['uid_num_of_author_fft_var'].mean())\n",
    "\n",
    "    group=data[['uid','author_id']].groupby('uid')\n",
    "    group=group.apply(lambda x:np.var(list(Counter(list(x['author_id'])).values())))\n",
    "    train_df['uid_num_of_author_var']=train_df['uid'].apply(lambda x:group[x])\n",
    "    test_df['uid_num_of_author_var']=test_df['uid'].apply(lambda x:group[x])\n",
    "    print(train_df[train_df['finish']==1]['uid_num_of_author_var'].mean())\n",
    "    print(train_df[train_df['finish']==0]['uid_num_of_author_var'].mean())\n",
    "\n",
    "    group=data[['uid','author_id']].groupby('uid')\n",
    "    group=group.apply(lambda x:np.mean(list(Counter(list(x['author_id'])).values())))\n",
    "    train_df['uid_num_of_author_mean']=train_df['uid'].apply(lambda x:group[x])\n",
    "    test_df['uid_num_of_author_mean']=test_df['uid'].apply(lambda x:group[x])\n",
    "    print(train_df[train_df['finish']==1]['uid_num_of_author_mean'].mean())\n",
    "    print(train_df[train_df['finish']==0]['uid_num_of_author_mean'].mean())\n",
    "    \n",
    "    \n",
    "def did_features(train_df,test_df):\n",
    "    data=train_df[['uid','did','author_id']].append(test_df[['uid','did','author_id']])\n",
    "\n",
    "    group=data[['uid','did']].groupby('uid')\n",
    "    group=group.apply(lambda x: len(set(x['did'])))\n",
    "    train_df['uid_has_num_of_did']=train_df['uid'].apply(lambda x: group[x])\n",
    "    test_df['uid_has_num_of_did']=test_df['uid'].apply(lambda x: group[x])\n",
    "\n",
    "    group=data[['did','author_id']].groupby('did')\n",
    "    group=group.apply(lambda x: len(set(x['author_id'])))\n",
    "    train_df['did_has_num_of_author']=train_df['did'].apply(lambda x: group[x])\n",
    "    test_df['did_has_num_of_author']=test_df['did'].apply(lambda x: group[x])\n",
    "\n",
    "def author_features(train_df,test_df):\n",
    "    data=train_df.append(test_df)\n",
    "    groupby=data[['author_id','item_id']].drop_duplicates().groupby('author_id')\n",
    "    groupby=groupby.apply(lambda x: len(set(x['item_id'])))\n",
    "    print('author_include_num_of_item')\n",
    "    train_df['author_include_num_of_item']=train_df['author_id'].apply(lambda x:groupby[x])\n",
    "    test_df['author_include_num_of_item']=test_df['author_id'].apply(lambda x:groupby[x])\n",
    "    print('avg of author_include_num_of_item',np.mean(train_df['author_include_num_of_item']),\\\n",
    "          np.mean(test_df['author_include_num_of_item']))  \n",
    "    \n",
    "    groupby=data[['author_id','uid']].drop_duplicates().groupby('author_id')\n",
    "    groupby=groupby.apply(lambda x: len(set(x['uid'])))\n",
    "    print('author_include_num_of_uid')\n",
    "    train_df['author_include_num_of_uid']=train_df['author_id'].apply(lambda x:groupby[x])\n",
    "    test_df['author_include_num_of_uid']=test_df['author_id'].apply(lambda x:groupby[x])\n",
    "    print('avg of author_include_num_of_uid',np.mean(train_df['author_include_num_of_uid']),\\\n",
    "          np.mean(test_df['author_include_num_of_uid']))        \n",
    "def uid_author_01(train_df,test_df):\n",
    "    dic_1={}\n",
    "    dic_2={}\n",
    "    dic={}\n",
    "    temp=[]\n",
    "    day=0\n",
    "    for item in train_df[['uid','author_id','finish','day']].values:\n",
    "        if day!=item[3]:\n",
    "            dic_1=dic_2.copy()\n",
    "            dic_2=dic.copy()\n",
    "            day=item[3]\n",
    "            print(day)\n",
    "        t=tuple(item[:2])\n",
    "        try:\n",
    "            if item[2]==0:\n",
    "                dic[t]+='0'\n",
    "            else:\n",
    "                dic[t]+='1'\n",
    "        except:\n",
    "            dic[t]=''\n",
    "            if item[2]==0:\n",
    "                dic[t]+='0'\n",
    "            else:\n",
    "                dic[t]+='1'        \n",
    "        try:\n",
    "            temp.append(dic_1[t][-4:])\n",
    "        except:\n",
    "            temp.append('-1')\n",
    "\n",
    "    train_df['01']=temp\n",
    "    total=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','author_id']].values:\n",
    "        item=tuple(item)\n",
    "        try:\n",
    "            temp.append(dic[item][-4:])\n",
    "        except:\n",
    "            temp.append('-1')\n",
    "    test_df['01']=temp\n",
    "    total+=temp\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(total)\n",
    "    train_df['01'] = lbl.transform(train_df['01'])\n",
    "    test_df['01'] = lbl.transform(test_df['01'])\n",
    "    \n",
    "def last_author_id(train_df,test_df):\n",
    "    dic={}\n",
    "    temp=[]\n",
    "    for item in train_df[['uid','author_id']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    train_df['last_author']=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','author_id']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    test_df['last_author']=temp    \n",
    "    \n",
    "def last_item_id(train_df,test_df):\n",
    "    dic={}\n",
    "    temp=[]\n",
    "    for item in train_df[['uid','item_id']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    train_df['last_item_id']=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','item_id']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    test_df['last_item_id']=temp     \n",
    "    \n",
    "def last_did(train_df,test_df):\n",
    "    dic={}\n",
    "    temp=[]\n",
    "    for item in train_df[['uid','did']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    train_df['last_did']=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','did']].values:\n",
    "        try:\n",
    "            temp.append(dic[item[0]])\n",
    "        except:\n",
    "            temp.append(-1)\n",
    "        dic[item[0]]=item[1]\n",
    "    test_df['last_did']=temp  \n",
    "    \n",
    "class BayesianSmoothing(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def update(self, imps, clks, iter_num, epsilon):\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "            if abs(new_alpha - self.alpha) < epsilon and abs(new_beta - self.beta) < epsilon:\n",
    "                break\n",
    "            print(new_alpha, new_beta, i)\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "        numerator_alpha = 0.0\n",
    "        numerator_beta = 0.0\n",
    "        denominator = 0.0\n",
    "        for i in range(len(imps)):\n",
    "            numerator_alpha += (special.digamma(clks[i] + alpha) - special.digamma(alpha))\n",
    "            numerator_beta += (special.digamma(imps[i] - clks[i] + beta) - special.digamma(beta))\n",
    "            denominator += (special.digamma(imps[i] + alpha + beta) - special.digamma(alpha + beta))\n",
    "\n",
    "        return alpha * (numerator_alpha / denominator), beta * (numerator_beta / denominator)\n",
    "    \n",
    "def rate_statistic(train_df,test_df,label,f,num):\n",
    "    bs = BayesianSmoothing(1, 1)\n",
    "    l = list(set(train_df[f].values))\n",
    "    dic = dict(Counter(train_df[f].values))\n",
    "    I = []\n",
    "    C = []\n",
    "    for idx in l:\n",
    "        I.append(dic[idx])\n",
    "    positive = train_df[train_df[label] == 1]\n",
    "    dic2 = dict(Counter(positive[f].values))\n",
    "    for idx in l:\n",
    "        if idx not in dic2:\n",
    "            C.append(0)\n",
    "        else:\n",
    "            C.append(dic2[idx])\n",
    "    bs.update(I, C, num, 0.0000000001)\n",
    "    alpha, beta=bs.alpha, bs.beta\n",
    "    \n",
    "    groupby={}\n",
    "    for item in train_df[[f,label]].values:\n",
    "        try:\n",
    "            groupby[item[0]][item[1]]+=1\n",
    "        except:\n",
    "            groupby[item[0]]=[0,0]\n",
    "            groupby[item[0]][item[1]]+=1\n",
    "    a = []\n",
    "    b = []\n",
    "    for i in train_df[f].values:\n",
    "        try:\n",
    "            label1= groupby[i][1]\n",
    "            label0= groupby[i][0]\n",
    "        except:\n",
    "            label1 = 0\n",
    "            label0 = 0\n",
    "        a.append((label1 + alpha) / (label0 + label1 + alpha + beta))\n",
    "    for i in test_df[f].values:\n",
    "        try:\n",
    "            label1= groupby[i][1]\n",
    "            label0= groupby[i][0]\n",
    "        except:\n",
    "            label1 = 0\n",
    "            label0 = 0\n",
    "        b.append((label1 + alpha) / (label0 + label1 + alpha + beta))\n",
    "    train_df[f +'_'+label+ '_smooth_rate'] = a\n",
    "    test_df[f + '_'+label+'_smooth_rate'] = b\n",
    "    print(f +'_'+label+ '_smooth_rate')\n",
    "    print(train_df[f +'_'+label+ '_smooth_rate'].head())\n",
    "    print(train_df[f +'_'+label+ '_smooth_rate'].mean(),test_df[f +'_'+label+ '_smooth_rate'].mean())  \n",
    "  \n",
    "def norm(train_df,test_df,features):   \n",
    "    df=pd.concat([train_df,test_df])[features]\n",
    "    scaler = preprocessing.QuantileTransformer(random_state=0)\n",
    "    scaler.fit(df[features]) \n",
    "    train_df[features]=scaler.transform(train_df[features])\n",
    "    test_df[features]=scaler.transform(test_df[features])\n",
    "\n",
    "def norm(df,features):   \n",
    "    scaler.fit(df[features]) \n",
    "    df[features]=scaler.transform(df[features])\n",
    "\n",
    "        \n",
    "def day_ratio(train_df,test_df,f1,f2):\n",
    "    train_df['f1']=train_df[f1]\n",
    "    train_df['f2']=train_df[f2]*1e10+train_df[f1]\n",
    "    test_df['f1']=test_df[f1]\n",
    "    test_df['f2']=test_df[f2]*1e10+test_df[f1] \n",
    "    day_cont(train_df,test_df,'f1')\n",
    "    day_cont(train_df,test_df,'f2')\n",
    "    train_df[f1+'_'+f2+'_day_ratio']=train_df['f2_day_cont']/train_df['f1_day_cont']\n",
    "    test_df[f1+'_'+f2+'_day_ratio']=test_df['f2_day_cont']/test_df['f1_day_cont']\n",
    "    del train_df['f1']\n",
    "    del train_df['f2']\n",
    "    del train_df['f1_day_cont']\n",
    "    del train_df['f2_day_cont']\n",
    "    del test_df['f1']\n",
    "    del test_df['f2']\n",
    "    del test_df['f1_day_cont']\n",
    "    del test_df['f2_day_cont']\n",
    "    gc.collect()\n",
    "    norm(train_df,[f1+'_'+f2+'_day_ratio'])\n",
    "    norm(test_df,[f1+'_'+f2+'_day_ratio'])\n",
    "    print(f1+'_'+f2+'_day_ratio', 'done!')\n",
    "    print(train_df[f1+'_'+f2+'_day_ratio'].mean(),test_df[f1+'_'+f2+'_day_ratio'].mean())\n",
    "    \n",
    "def all_ratio(train_df,test_df,f1,f2):\n",
    "    train_df['f1']=train_df[f1]\n",
    "    train_df['f2']=train_df[f2]*1e8+train_df[f1]\n",
    "    test_df['f1']=test_df[f1]\n",
    "    test_df['f2']=test_df[f2]*1e8+test_df[f1] \n",
    "    all_cont(train_df,test_df,'f1')\n",
    "    all_cont(train_df,test_df,'f2')\n",
    "    train_df[f1+'_'+f2+'_all_ratio']=train_df['f2_all_cont']/train_df['f1_all_cont']\n",
    "    test_df[f1+'_'+f2+'_all_ratio']=test_df['f2_all_cont']/test_df['f1_all_cont']\n",
    "    del train_df['f1']\n",
    "    del train_df['f2']\n",
    "    del train_df['f1_all_cont']\n",
    "    del train_df['f2_all_cont']\n",
    "    del test_df['f1']\n",
    "    del test_df['f2']\n",
    "    del test_df['f1_all_cont']\n",
    "    del test_df['f2_all_cont']\n",
    "    gc.collect()\n",
    "    norm(train_df,[f1+'_'+f2+'_all_ratio'])\n",
    "    norm(test_df,[f1+'_'+f2+'_all_ratio'])\n",
    "    print(f1+'_'+f2+'_all_ratio', 'done!')\n",
    "    print(train_df[f1+'_'+f2+'_all_ratio'].mean(),test_df[f1+'_'+f2+'_all_ratio'].mean())   \n",
    "\n",
    "def uid_item_time_stamp(train_df,test_df):\n",
    "    data=train_df.append(test_df)\n",
    "    groupby=data.groupby('uid')['item_id'].mean()\n",
    "    train_df['mean_id']=train_df['uid'].apply(lambda x: groupby[x])\n",
    "    test_df['mean_id']=test_df['uid'].apply(lambda x: groupby[x])\n",
    "    train_df['id_mean_differ']=train_df['item_id']-train_df['mean_id']\n",
    "    test_df['id_mean_differ']=test_df['item_id']-test_df['mean_id']\n",
    "    print(\"done 1!\")\n",
    "    groupby=data.groupby('uid')['item_id'].min()\n",
    "    train_df['min_id']=train_df['uid'].apply(lambda x: groupby[x])\n",
    "    test_df['min_id']=test_df['uid'].apply(lambda x: groupby[x])\n",
    "    train_df['id_min_differ']=train_df['item_id']-train_df['min_id']\n",
    "    test_df['id_min_differ']=test_df['item_id']-test_df['min_id']\n",
    "    print(\"done 2!\")\n",
    "\n",
    "    groupby=data.groupby('uid')['item_id'].max()\n",
    "    train_df['max_id']=train_df['uid'].apply(lambda x: groupby[x])\n",
    "    test_df['max_id']=test_df['uid'].apply(lambda x: groupby[x])\n",
    "    train_df['id_max_differ']=train_df['item_id']-train_df['max_id']\n",
    "    test_df['id_max_differ']=test_df['item_id']-test_df['max_id']\n",
    "    print(\"done 3!\")\n",
    "    \n",
    "def deepwalk(train_df,test_df,f1,f2,flag):\n",
    "    print(\"deepwalk:\",f1,f2)\n",
    "    L=10\n",
    "    dic={}\n",
    "\n",
    "    for item in train_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic['item_'+str(item[1])].add('user_'+str(item[0]))\n",
    "        except:\n",
    "            dic['item_'+str(item[1])]=set(['user_'+str(item[0])])\n",
    "        try:\n",
    "            dic['user_'+str(item[0])].add('item_'+str(item[1]))\n",
    "        except:\n",
    "            dic['user_'+str(item[0])]=set(['item_'+str(item[1])])\n",
    "\n",
    "    for item in test_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic['item_'+str(item[1])].add('user_'+str(item[0]))\n",
    "        except:\n",
    "            dic['item_'+str(item[1])]=set(['user_'+str(item[0])])\n",
    "        try:\n",
    "            dic['user_'+str(item[0])].add('item_'+str(item[1]))\n",
    "        except:\n",
    "            dic['user_'+str(item[0])]=set(['item_'+str(item[1])])\n",
    "    print(\"creating\")        \n",
    "    path_length=10        \n",
    "    sentences=[]\n",
    "    length=[]\n",
    "    for key in dic:\n",
    "        sentence=[key]\n",
    "        while len(sentence)!=path_length:\n",
    "            key=random.sample(dic[sentence[-1]],1)[0]\n",
    "            if len(sentence)>=2 and key == sentence[-2]:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(key)\n",
    "\n",
    "        sentences.append(sentence)\n",
    "        length.append(len(sentence))\n",
    "        if len(sentences)%100000==0:\n",
    "            print(len(sentences))\n",
    "    print(np.mean(length))\n",
    "    print(len(sentences))\n",
    "    print('training...')\n",
    "    random.shuffle(sentences)\n",
    "    model = Word2Vec(sentences, size=L, window=4, min_count=1, workers=10,iter=30)\n",
    "    print('outputing...')\n",
    "    \n",
    "    values=set(train_df[f1].values)|set(test_df[f1].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model['user_'+str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f1]\n",
    "    for i in range(L):\n",
    "        names.append(names[0]+'_deepwalk_embedding_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' + f1 +'_'+flag +'_deepwalk.pkl') \n",
    "    \n",
    "    values=set(train_df[f2].values)|set(test_df[f2].values)\n",
    "    w2v=[]\n",
    "    for v in values:\n",
    "        a=[v]\n",
    "        a.extend(model['item_'+str(v)])\n",
    "        w2v.append(a)\n",
    "    out_df=pd.DataFrame(w2v)\n",
    "    names=[f2]\n",
    "    for i in range(L):\n",
    "        names.append(names[0]+'_deepwalk_embedding_'+str(i))\n",
    "    out_df.columns = names\n",
    "    print(out_df.head())\n",
    "    out_df.to_pickle('data/' + f2 +'_'+flag +'_deepwalk.pkl') \n",
    "    \n",
    "def cosine(train_df,test_df,f1,f2):\n",
    "    print(\"cosine:\",f1,f2)\n",
    "    L=16\n",
    "    dic={}\n",
    "\n",
    "    for item in train_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic['item_'+str(item[1])].add('user_'+str(item[0]))\n",
    "        except:\n",
    "            dic['item_'+str(item[1])]=set(['user_'+str(item[0])])\n",
    "        try:\n",
    "            dic['user_'+str(item[0])].add('item_'+str(item[1]))\n",
    "        except:\n",
    "            dic['user_'+str(item[0])]=set(['item_'+str(item[1])])\n",
    "\n",
    "    for item in test_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic['item_'+str(item[1])].add('user_'+str(item[0]))\n",
    "        except:\n",
    "            dic['item_'+str(item[1])]=set(['user_'+str(item[0])])\n",
    "        try:\n",
    "            dic['user_'+str(item[0])].add('item_'+str(item[1]))\n",
    "        except:\n",
    "            dic['user_'+str(item[0])]=set(['item_'+str(item[1])])\n",
    "    print(\"creating\")        \n",
    "    path_length=10        \n",
    "    sentences=[]\n",
    "    length=[]\n",
    "    for key in dic:\n",
    "        sentence=[key]\n",
    "        while len(sentence)!=path_length:\n",
    "            key=random.sample(dic[sentence[-1]],1)[0]\n",
    "            if len(sentence)>=2 and key == sentence[-2]:\n",
    "                break\n",
    "            else:\n",
    "                sentence.append(key)\n",
    "\n",
    "        sentences.append(sentence)\n",
    "        length.append(len(sentence))\n",
    "        if len(sentences)%100000==0:\n",
    "            print(len(sentences))\n",
    "    print(np.mean(length))\n",
    "    print(len(sentences))\n",
    "    print('training...')\n",
    "    random.shuffle(sentences)\n",
    "    model = Word2Vec(sentences, size=L, window=4, min_count=1, workers=10,iter=1)\n",
    "    print('outputing...')\n",
    "    temp=[]\n",
    "    for item in train_df[['uid','item_id']].values:\n",
    "        temp.append(model.similarity('user_'+str(item[0]),'item_'+str(item[1])))\n",
    "    train_df[f1+'_'+f2+'_cosine']=temp\n",
    "    temp=[]\n",
    "    for item in test_df[['uid','item_id']].values:\n",
    "        temp.append(model.similarity('user_'+str(item[0]),'item_'+str(item[1])))\n",
    "    test_df[f1+'_'+f2+'_cosine']=temp    \n",
    "    print(f1+'_'+f2+'_cosine')\n",
    "    print(train_df[train_df['finish']==1][f1+'_'+f2+'_cosine'].mean(),train_df[train_df['finish']==0][f1+'_'+f2+'_cosine'].mean())\n",
    "    \n",
    "def titile_kfold_static(train_df,test_df,label):\n",
    "    print(\"K-fold static:\",'title_'+label)\n",
    "    #K-fold positive and negative num\n",
    "    avg_rate=train_df[label].mean()\n",
    "    num=len(train_df)//5\n",
    "    index=[0 for i in range(num)]+[1 for i in range(num)]+[2 for i in range(num)]+[3 for i in range(num)]+[4 for i in range(len(train_df)-4*num)]\n",
    "    random.shuffle(index)\n",
    "    train_df['index']=index\n",
    "\n",
    "        \n",
    "\n",
    "    dic=[{} for i in range(5)]\n",
    "    dic_all={}\n",
    "    for item in train_df[['index','title_keys',label]].values:\n",
    "        for t in item[1].split():\n",
    "            try:\n",
    "                dic[item[0]][t][item[2]]+=1\n",
    "            except:\n",
    "                dic[item[0]][t]=[0,0]\n",
    "                dic[item[0]][t][item[2]]+=1\n",
    "            try:\n",
    "                dic_all[t][item[2]]+=1\n",
    "            except:\n",
    "                dic_all[t]=[0,0]\n",
    "                dic_all[t][item[2]]+=1\n",
    "    print(\"static done!\")\n",
    "                \n",
    "    max_rate=[]\n",
    "    min_rate=[]\n",
    "    mean_rate=[]\n",
    "    for item in train_df[['index','title_keys']].values:\n",
    "        temp=[]\n",
    "        for t in item[1].split():\n",
    "            n,p=dic_all[t]\n",
    "            try:\n",
    "                p-=dic[item[0]][t][1]\n",
    "                n-=dic[item[0]][t][0] \n",
    "            except:\n",
    "                pass\n",
    "            if p==0 and n==0:\n",
    "                temp.append(avg_rate)\n",
    "            else:\n",
    "                temp.append(p/(p+n))  \n",
    "        max_rate.append(max(temp))\n",
    "        min_rate.append(min(temp))\n",
    "        mean_rate.append(np.mean(temp))\n",
    "    \n",
    "            \n",
    "\n",
    "    train_df['title_max_rate_'+label]=max_rate\n",
    "    train_df['title_min_rate_'+label]=min_rate\n",
    "    train_df['title_mean_rate_'+label]=mean_rate\n",
    "    \n",
    "    print(\"train done!\")\n",
    "    #for test\n",
    "    max_rate=[]\n",
    "    min_rate=[]\n",
    "    mean_rate=[]\n",
    "    for uid in test_df['title_keys'].values:\n",
    "        temp=[]\n",
    "        for t in uid.split():\n",
    "            p=0\n",
    "            n=0\n",
    "            try:\n",
    "                p=dic_all[t][1]\n",
    "                n=dic_all[t][0]\n",
    "            except:\n",
    "                pass\n",
    "            if p==0 and n==0:\n",
    "                temp.append(avg_rate)\n",
    "            else:\n",
    "                temp.append(p/(p+n))  \n",
    "        max_rate.append(max(temp))\n",
    "        min_rate.append(min(temp))\n",
    "        mean_rate.append(np.mean(temp))\n",
    "        \n",
    "    test_df['title_max_rate_'+label]=max_rate\n",
    "    test_df['title_min_rate_'+label]=min_rate\n",
    "    test_df['title_mean_rate_'+label]=mean_rate\n",
    "    print(\"test done!\")\n",
    "    del train_df['index']\n",
    "    print('title_max_rate_'+label)\n",
    "    print('title_min_rate_'+label)\n",
    "    print('title_mean_rate_'+label)\n",
    "    print('avg of max rate',np.mean(train_df['title_max_rate_'+label]),np.mean(test_df['title_max_rate_'+label]))\n",
    "    print('avg of min rate',np.mean(train_df['title_min_rate_'+label]),np.mean(test_df['title_min_rate_'+label]))\n",
    "    print('avg of mean rate',np.mean(train_df['title_mean_rate_'+label]),np.mean(test_df['title_mean_rate_'+label]))\n",
    "\n",
    "def get_agg_features(train_df,test_df,f1,f2,agg):\n",
    "    if type(f1)==str:\n",
    "        f1=[f1]\n",
    "    data=train_df[f1+[f2]].append(test_df[f1+[f2]])\n",
    "    if agg == \"count\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].count()).reset_index()\n",
    "    elif agg==\"mean\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].mean()).reset_index()\n",
    "    elif agg==\"nunique\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].nunique()).reset_index()\n",
    "    elif agg==\"max\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].max()).reset_index()\n",
    "    elif agg==\"min\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].min()).reset_index()\n",
    "    elif agg==\"sum\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].sum()).reset_index()\n",
    "    elif agg==\"std\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].std()).reset_index()\n",
    "    elif agg==\"median\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].median()).reset_index()\n",
    "    elif agg==\"skew\":\n",
    "        tmp = pd.DataFrame(data.groupby(f1)[f2].skew()).reset_index()\n",
    "    elif agg==\"unique_mean\":\n",
    "        group=data.groupby(f1)\n",
    "        group=group.apply(lambda x:np.mean(list(Counter(list(x[f2])).values())))\n",
    "        tmp = pd.DataFrame(group.reset_index())\n",
    "    elif agg==\"unique_var\":\n",
    "        group=data.groupby(f1)\n",
    "        group=group.apply(lambda x:np.var(list(Counter(list(x[f2])).values())))\n",
    "        tmp = pd.DataFrame(group.reset_index())\n",
    "    else:\n",
    "        raise \"agg error\"\n",
    "    tmp.columns = f1+['_'.join(f1)+\"_\"+f2+\"_\"+agg]\n",
    "    print('_'.join(f1)+\"_\"+f2+\"_\"+agg)\n",
    "    train_df=train_df.merge(tmp, on=f1, how='left')\n",
    "    test_df=test_df.merge(tmp, on=f1, how='left')\n",
    "    del tmp\n",
    "    del data\n",
    "    gc.collect()\n",
    "    print(train_df.shape,test_df.shape)\n",
    "    return train_df,test_df    \n",
    "def topk_id(train_df,test_df,f1,f2,k):\n",
    "    dic={}\n",
    "    for item in train_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic[item[0]][item[1]]+=1\n",
    "        except:\n",
    "            dic[item[0]]=Counter()\n",
    "            dic[item[0]][item[1]]+=1\n",
    "    for item in test_df[[f1,f2]].values:\n",
    "        try:\n",
    "            dic[item[0]][item[1]]+=1\n",
    "        except:\n",
    "            dic[item[0]]=Counter()\n",
    "            dic[item[0]][item[1]]+=1\n",
    "    temp=[]\n",
    "    count=[]\n",
    "    result={}\n",
    "    for item in train_df[f1].values:\n",
    "        tmp=[]\n",
    "        cont=[]\n",
    "        try:\n",
    "            tmp=result[item]\n",
    "            cont=result[item]\n",
    "        except:\n",
    "            for t in dic[item].most_common(k):\n",
    "                tmp.append(t[0])\n",
    "                cont.append(t[1])\n",
    "            if len(tmp)!=k:\n",
    "                tmp+=[-1]*(k-len(tmp))\n",
    "                cont+=[-1]*(k-len(cont))\n",
    "            result[item]=tmp\n",
    "            result[item]=cont\n",
    "        temp.append(tmp)\n",
    "        count.append(cont)\n",
    "    df1=pd.DataFrame(temp)\n",
    "    df1.columns=[f1+'_'+f2+'_top'+str(i) for i in range(k)]\n",
    "    df2=pd.DataFrame(count)\n",
    "    df2.columns=[f1+'_'+f2+'_top'+str(i)+'_count' for i in range(k)]   \n",
    "    try:\n",
    "        for f in [f1+'_'+f2+'_top'+str(i) for i in range(k)]:\n",
    "            del train_df[f]\n",
    "            del test_df[f]\n",
    "        for f in [f1+'_'+f2+'_top'+str(i)+'_count' for i in range(k)]:\n",
    "            del train_df[f]\n",
    "            del test_df[f]\n",
    "    except:\n",
    "        pass\n",
    "    train_df=pd.concat([train_df,df1],1)\n",
    "    train_df=pd.concat([train_df,df2],1)\n",
    "    del temp\n",
    "    del count\n",
    "    gc.collect()\n",
    "    print(\"train done!\")\n",
    "    temp=[]\n",
    "    count=[]\n",
    "    for item in test_df[f1].values:\n",
    "        tmp=[]\n",
    "        cont=[]\n",
    "        try:\n",
    "            tmp=result[item]\n",
    "            cont=result[item]\n",
    "        except:\n",
    "            for t in dic[item].most_common(k):\n",
    "                tmp.append(t[0])\n",
    "                cont.append(t[1])\n",
    "            if len(tmp)!=k:\n",
    "                tmp+=[-1]*(k-len(tmp))\n",
    "                cont+=[-1]*(k-len(cont))\n",
    "            result[item]=tmp\n",
    "            result[item]=cont\n",
    "            \n",
    "        temp.append(tmp)\n",
    "        count.append(cont)\n",
    "    print(\"test done!\")\n",
    "    df1=pd.DataFrame(temp)\n",
    "    df1.columns=[f1+'_'+f2+'_top'+str(i) for i in range(k)]\n",
    "    df2=pd.DataFrame(count)\n",
    "    df2.columns=[f1+'_'+f2+'_top'+str(i)+'_count' for i in range(k)]    \n",
    "    test_df=pd.concat([test_df,df1],1)\n",
    "    test_df=pd.concat([test_df,df2],1)\n",
    "    del temp\n",
    "    del count\n",
    "    gc.collect() \n",
    "    print([f1+'_'+f2+'_top'+str(i) for i in range(k)])\n",
    "    print([f1+'_'+f2+'_top'+str(i)+'_count' for i in range(k)] )\n",
    "    print(train_df.shape,test_df.shape)\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_dev.pkl data/dev.pkl\n",
      "(14015958, 131) (2803191, 131)\n",
      "train done!\n",
      "test done!\n",
      "['uid_item_id_top0', 'uid_item_id_top1']\n"
     ]
    }
   ],
   "source": [
    "for path1,path2,flag in [('data/train_dev.pkl','data/dev.pkl','dev')]:\n",
    "        print(path1,path2)\n",
    "        train_df=pd.read_pickle(path1)\n",
    "        test_df=pd.read_pickle(path2) \n",
    "       # w2v(train_df,test_df,'item_id',flag)\n",
    "        print(train_df.shape,test_df.shape)\n",
    "        #last_did(train_df,test_df)\n",
    "        \"\"\"\n",
    "        last_author_id(train_df,test_df)\n",
    "        last_item_id(train_df,test_df)\n",
    "\n",
    "        for f in ['item_id','author_id','did']:\n",
    "            kfold_static(train_df,test_df,f,'finish') \n",
    "        for f in ['item_id','author_id','did','uid','user_city','item_city','music_id']:\n",
    "            kfold_static(train_df,test_df,f,'like')    \n",
    "        for f in ['author_id']:\n",
    "            w2v(train_df,test_df,f,flag)\n",
    "        for f in ['uid','did']:\n",
    "            w2v_1(train_df,test_df,'uid',flag)\n",
    "            w2v_1(train_df,test_df,'did',flag)\n",
    "        \n",
    "        combine(train_df,test_df,'author_id','did')\n",
    "        for f in ['did','author_id','item_id','author_id_did']:\n",
    "            all_cont(train_df,test_df,f)\n",
    "            day_cont(train_df,test_df,f)\n",
    "\n",
    "        var_mean(train_df,test_df)\n",
    "        author_features(train_df,test_df)\n",
    "        did_features(train_df,test_df) \n",
    "        kfold_static(train_df,test_df,'last_item_id','finish')\n",
    "        kfold_static(train_df,test_df,'last_author','finish')\n",
    "        \"\"\"\n",
    "        #deepwalk(train_df,test_df,'uid','item_id',flag)\n",
    "        \"\"\"\n",
    "        for f1,f2 in [('uid','author_id'),('uid','music_id'),('uid','item_id'),('uid','item_city'),('did','author_id'),('did','music_id'),('did','item_id'),('did','item_city'),\n",
    "                     ('item_id','uid'),('item_id','did'),('item_id','user_city'),('item_id','channel'),('author_id','uid'),('author_id','did'),('author_id','user_city'),('author_id','item_id'),('author_id','channel'),\n",
    "                     ('channel','uid'),('channel','did'),('channel','item_id'),('channel','author_id')]:\n",
    "            day_ratio(train_df,test_df,f1,f2)\n",
    "            all_ratio(train_df,test_df,f1,f2)\n",
    "        \"\"\"\n",
    "        train_df,test_df=topk_id(train_df,test_df,'uid','item_id',2)\n",
    "        train_df,test_df=topk_id(train_df,test_df,'uid','author_id',2)\n",
    "        train_df,test_df=topk_id(train_df,test_df,'uid','music_id',2)\n",
    "        train_df,test_df=topk_id(train_df,test_df,'uid','channel',2)\n",
    "        train_df,test_df=topk_id(train_df,test_df,'uid','music_id',2)\n",
    "        train_df,test_df=topk_id(train_df,test_df,'uid','did',2)\n",
    "        train_df,test_df=topk_id(train_df,test_df,'uid','video_duration',2)\n",
    "        \n",
    "        print(train_df.shape,test_df.shape)\n",
    "        print(list(train_df))\n",
    "        train_df.to_pickle(path1) \n",
    "        test_df.to_pickle(path2)  \n",
    "        print(\"*\"*80)\n",
    "        print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1322"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['did_author_id_top0_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_pickle('data/train_dev.pkl')\n",
    "test_df=pd.read_pickle('data/dev.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uid',\n",
       " 'user_city',\n",
       " 'item_id',\n",
       " 'author_id',\n",
       " 'item_city',\n",
       " 'channel',\n",
       " 'finish',\n",
       " 'like',\n",
       " 'music_id',\n",
       " 'did',\n",
       " 'create_time',\n",
       " 'video_duration',\n",
       " 'day',\n",
       " 'title_keys',\n",
       " 'title_values',\n",
       " 'title_cont',\n",
       " 'beauty_min',\n",
       " 'last_author',\n",
       " 'last_item_id',\n",
       " 'item_id_finish_positive_num',\n",
       " 'item_id_finish_negative_num',\n",
       " 'item_id_finish_rate',\n",
       " 'author_id_finish_positive_num',\n",
       " 'author_id_finish_negative_num',\n",
       " 'author_id_finish_rate',\n",
       " 'did_finish_positive_num',\n",
       " 'did_finish_negative_num',\n",
       " 'did_finish_rate',\n",
       " 'item_id_like_positive_num',\n",
       " 'item_id_like_negative_num',\n",
       " 'item_id_like_rate',\n",
       " 'author_id_like_positive_num',\n",
       " 'author_id_like_negative_num',\n",
       " 'author_id_like_rate',\n",
       " 'did_like_positive_num',\n",
       " 'did_like_negative_num',\n",
       " 'did_like_rate',\n",
       " 'uid_like_positive_num',\n",
       " 'uid_like_negative_num',\n",
       " 'uid_like_rate',\n",
       " 'user_city_like_positive_num',\n",
       " 'user_city_like_negative_num',\n",
       " 'user_city_like_rate',\n",
       " 'item_city_like_positive_num',\n",
       " 'item_city_like_negative_num',\n",
       " 'item_city_like_rate',\n",
       " 'music_id_like_positive_num',\n",
       " 'music_id_like_negative_num',\n",
       " 'music_id_like_rate',\n",
       " 'author_id_did',\n",
       " 'did_all_cont',\n",
       " 'did_day_cont',\n",
       " 'author_id_all_cont',\n",
       " 'author_id_day_cont',\n",
       " 'item_id_all_cont',\n",
       " 'item_id_day_cont',\n",
       " 'author_id_did_all_cont',\n",
       " 'author_id_did_day_cont',\n",
       " 'uid_num_of_author_fft_var',\n",
       " 'uid_num_of_author_var',\n",
       " 'uid_num_of_author_mean',\n",
       " 'author_include_num_of_item',\n",
       " 'author_include_num_of_uid',\n",
       " 'uid_has_num_of_did',\n",
       " 'did_has_num_of_author',\n",
       " 'mean_id',\n",
       " 'id_mean_differ',\n",
       " 'min_id',\n",
       " 'id_min_differ',\n",
       " 'max_id',\n",
       " 'id_max_differ',\n",
       " 'title_max_rate_finish',\n",
       " 'title_min_rate_finish',\n",
       " 'title_mean_rate_finish',\n",
       " 'title_max_rate_like',\n",
       " 'title_min_rate_like',\n",
       " 'title_mean_rate_like',\n",
       " 'uid_did_nunique_x',\n",
       " 'uid_did_count_x',\n",
       " 'uid_channel_nunique_x',\n",
       " 'did_video_duration_min_x',\n",
       " 'did_video_duration_max_x',\n",
       " 'did_video_duration_mean_x',\n",
       " 'did_video_duration_std_x',\n",
       " 'channel_video_duration_min_x',\n",
       " 'channel_video_duration_max_x',\n",
       " 'channel_video_duration_mean_x',\n",
       " 'channel_video_duration_std_x',\n",
       " 'uid_item_id_unique_mean',\n",
       " 'uid_author_id_unique_mean',\n",
       " 'uid_channel_unique_mean',\n",
       " 'did_item_id_unique_mean',\n",
       " 'did_author_id_unique_mean',\n",
       " 'did_channel_unique_mean',\n",
       " 'uid_item_id_unique_var',\n",
       " 'uid_author_id_unique_var',\n",
       " 'uid_channel_unique_var',\n",
       " 'did_item_id_unique_var',\n",
       " 'did_author_id_unique_var',\n",
       " 'did_channel_unique_var',\n",
       " 'author_id_title_cont_skew',\n",
       " 'author_id_title_cont_mean',\n",
       " 'author_id_title_cont_std',\n",
       " 'did_title_cont_skew',\n",
       " 'did_title_cont_mean',\n",
       " 'did_title_cont_std',\n",
       " 'uid_channel_title_cont_skew',\n",
       " 'uid_channel_title_cont_mean',\n",
       " 'uid_channel_title_cont_std',\n",
       " 'uid_did_nunique_y',\n",
       " 'uid_did_count_y',\n",
       " 'uid_channel_nunique_y',\n",
       " 'did_video_duration_min_y',\n",
       " 'did_video_duration_max_y',\n",
       " 'did_video_duration_mean_y',\n",
       " 'did_video_duration_std_y',\n",
       " 'channel_video_duration_min_y',\n",
       " 'channel_video_duration_max_y',\n",
       " 'channel_video_duration_mean_y',\n",
       " 'channel_video_duration_std_y',\n",
       " 'item_id_uid_nunique',\n",
       " 'item_id_uid_count',\n",
       " 'author_id_item_id_nunique',\n",
       " 'author_id_item_id_count',\n",
       " 'uid_user_city_nunique',\n",
       " 'uid_author_id_nunique',\n",
       " 'channel_user_city_nunique',\n",
       " 'did_video_duration_skew',\n",
       " 'channel_video_duration_skew']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in ['uid_did_nunique_y','uid_did_count_y', 'uid_channel_nunique_y',\n",
    " 'did_video_duration_min_y',\n",
    " 'did_video_duration_max_y',\n",
    " 'did_video_duration_mean_y',\n",
    " 'did_video_duration_std_y',\n",
    " 'channel_video_duration_min_y',\n",
    " 'channel_video_duration_max_y',\n",
    " 'channel_video_duration_mean_y',\n",
    " 'channel_video_duration_std_y',]:\n",
    "    del train_df[f]\n",
    "    del test_df[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uid', 'user_city', 'item_id', 'author_id', 'item_city', 'channel', 'finish', 'like', 'music_id', 'did', 'create_time', 'video_duration', 'day', 'title_keys', 'title_values', 'title_cont', 'beauty_min', 'last_author', 'last_item_id', 'item_id_finish_positive_num', 'item_id_finish_negative_num', 'item_id_finish_rate', 'author_id_finish_positive_num', 'author_id_finish_negative_num', 'author_id_finish_rate', 'did_finish_positive_num', 'did_finish_negative_num', 'did_finish_rate', 'item_id_like_positive_num', 'item_id_like_negative_num', 'item_id_like_rate', 'author_id_like_positive_num', 'author_id_like_negative_num', 'author_id_like_rate', 'did_like_positive_num', 'did_like_negative_num', 'did_like_rate', 'uid_like_positive_num', 'uid_like_negative_num', 'uid_like_rate', 'user_city_like_positive_num', 'user_city_like_negative_num', 'user_city_like_rate', 'item_city_like_positive_num', 'item_city_like_negative_num', 'item_city_like_rate', 'music_id_like_positive_num', 'music_id_like_negative_num', 'music_id_like_rate', 'author_id_did', 'did_all_cont', 'did_day_cont', 'author_id_all_cont', 'author_id_day_cont', 'item_id_all_cont', 'item_id_day_cont', 'author_id_did_all_cont', 'author_id_did_day_cont', 'uid_num_of_author_fft_var', 'uid_num_of_author_var', 'uid_num_of_author_mean', 'author_include_num_of_item', 'author_include_num_of_uid', 'uid_has_num_of_did', 'did_has_num_of_author', 'mean_id', 'id_mean_differ', 'min_id', 'id_min_differ', 'max_id', 'id_max_differ', 'title_max_rate_finish', 'title_min_rate_finish', 'title_mean_rate_finish', 'title_max_rate_like', 'title_min_rate_like', 'title_mean_rate_like', 'uid_did_nunique', 'uid_did_count', 'uid_channel_nunique', 'did_video_duration_min', 'did_video_duration_max', 'did_video_duration_mean', 'did_video_duration_std', 'channel_video_duration_min', 'channel_video_duration_max', 'channel_video_duration_mean', 'channel_video_duration_std', 'uid_item_id_unique_mean', 'uid_author_id_unique_mean', 'uid_channel_unique_mean', 'did_item_id_unique_mean', 'did_author_id_unique_mean', 'did_channel_unique_mean', 'uid_item_id_unique_var', 'uid_author_id_unique_var', 'uid_channel_unique_var', 'did_item_id_unique_var', 'did_author_id_unique_var', 'did_channel_unique_var', 'author_id_title_cont_skew', 'author_id_title_cont_mean', 'author_id_title_cont_std', 'did_title_cont_skew', 'did_title_cont_mean', 'did_title_cont_std', 'uid_channel_title_cont_skew', 'uid_channel_title_cont_mean', 'uid_channel_title_cont_std', 'item_id_uid_nunique', 'item_id_uid_count', 'author_id_item_id_nunique', 'author_id_item_id_count', 'uid_user_city_nunique', 'uid_author_id_nunique', 'channel_user_city_nunique', 'did_video_duration_skew', 'channel_video_duration_skew']\n"
     ]
    }
   ],
   "source": [
    "print(list(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df.columns=['uid', 'user_city', 'item_id', 'author_id', 'item_city', 'channel', 'finish', 'like', 'music_id', 'did', 'create_time', 'video_duration', 'day', 'title_keys', 'title_values', 'title_cont', 'beauty_min', 'last_author', 'last_item_id', 'item_id_finish_positive_num', 'item_id_finish_negative_num', 'item_id_finish_rate', 'author_id_finish_positive_num', 'author_id_finish_negative_num', 'author_id_finish_rate', 'did_finish_positive_num', 'did_finish_negative_num', 'did_finish_rate', 'item_id_like_positive_num', 'item_id_like_negative_num', 'item_id_like_rate', 'author_id_like_positive_num', 'author_id_like_negative_num', 'author_id_like_rate', 'did_like_positive_num', 'did_like_negative_num', 'did_like_rate', 'uid_like_positive_num', 'uid_like_negative_num', 'uid_like_rate', 'user_city_like_positive_num', 'user_city_like_negative_num', 'user_city_like_rate', 'item_city_like_positive_num', 'item_city_like_negative_num', 'item_city_like_rate', 'music_id_like_positive_num', 'music_id_like_negative_num', 'music_id_like_rate', 'author_id_did', 'did_all_cont', 'did_day_cont', 'author_id_all_cont', 'author_id_day_cont', 'item_id_all_cont', 'item_id_day_cont', 'author_id_did_all_cont', 'author_id_did_day_cont', 'uid_num_of_author_fft_var', 'uid_num_of_author_var', 'uid_num_of_author_mean', 'author_include_num_of_item', 'author_include_num_of_uid', 'uid_has_num_of_did', 'did_has_num_of_author', 'mean_id', 'id_mean_differ', 'min_id', 'id_min_differ', 'max_id', 'id_max_differ', 'title_max_rate_finish', 'title_min_rate_finish', 'title_mean_rate_finish', 'title_max_rate_like', 'title_min_rate_like', 'title_mean_rate_like', 'uid_did_nunique', 'uid_did_count', 'uid_channel_nunique', 'did_video_duration_min', 'did_video_duration_max', 'did_video_duration_mean', 'did_video_duration_std', 'channel_video_duration_min', 'channel_video_duration_max', 'channel_video_duration_mean', 'channel_video_duration_std', 'uid_item_id_unique_mean', 'uid_author_id_unique_mean', 'uid_channel_unique_mean', 'did_item_id_unique_mean', 'did_author_id_unique_mean', 'did_channel_unique_mean', 'uid_item_id_unique_var', 'uid_author_id_unique_var', 'uid_channel_unique_var', 'did_item_id_unique_var', 'did_author_id_unique_var', 'did_channel_unique_var', 'author_id_title_cont_skew', 'author_id_title_cont_mean', 'author_id_title_cont_std', 'did_title_cont_skew', 'did_title_cont_mean', 'did_title_cont_std', 'uid_channel_title_cont_skew', 'uid_channel_title_cont_mean', 'uid_channel_title_cont_std', 'item_id_uid_nunique', 'item_id_uid_count', 'author_id_item_id_nunique', 'author_id_item_id_count', 'uid_user_city_nunique', 'uid_author_id_nunique', 'channel_user_city_nunique', 'did_video_duration_skew', 'channel_video_duration_skew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle('data/train_dev.pkl') \n",
    "test_df.to_pickle('data/dev.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
